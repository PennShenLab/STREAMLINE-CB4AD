{"cells":[{"cell_type":"markdown","metadata":{"id":"U96A49858xZ-"},"source":["![STREAMLINE_LOGO.png](attachment:STREAMLINE_LOGO.png)"]},{"cell_type":"markdown","metadata":{"id":"WEkqGo2J8xaB"},"source":["# Summary"]},{"cell_type":"markdown","metadata":{"id":"vhyMhEPR8xaB"},"source":["This notebook runs all aspects of the STREAMLINE which is an automated machine learning analysis pipeline for binary classification tasks. Of note, two potentially important elements that are not automated by this pipeline include careful data cleaning and feature engineering using problem domain knowledge. Please review the README included in the associated GitHub repository for a detailed overview of how to run this pipeline. For simplicity, this notebook runs Python code outside of what is visible within it. \n","\n","This notebook is set up to run 'as-is' on a 'demo' dataset from the UCI repository (HCC dataset) using only three modeling algorithms (so that it runs in a matter of minutes). We analyze a copy of the dataset with and without covariate features to show how this pipline can be run on multiple datasets simultaneously (having the option to compare modeling on these different datasets in a later phase of the pipeline. Users will need to update pipeline run parameters below to ready the pipeline for their own needs. Suggested default run parameters suitible for most users are included, however file paths and names will need to be edited to run anything other than the 'demo' analysis. "]},{"cell_type":"markdown","metadata":{"id":"BKSIoL7b8xaC"},"source":["## Notebook Housekeeping\n","Set up notebook cells to display desired results. No need to edit."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"c5pP76nX8xaC","executionInfo":{"status":"ok","timestamp":1653662238861,"user_tz":240,"elapsed":27,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import warnings\n","import sys\n","import os\n","warnings.filterwarnings('ignore')\n","\n","# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""]},{"cell_type":"markdown","metadata":{"id":"0PxI1ts58xaD"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## (User Specified) Run Parameters of STREAMLINE\n","These initial notebook cells include all customizable run parameters for STREAMLINE. These settings should only be left unchanged for users wishing to test out the pipeline demo (as is) to learn how it works or to confirm efficacy before running their own data. Run parameters for each phase of the pipeline are included in separate code cells of this section of the notebook.\n"]},{"cell_type":"markdown","metadata":{"id":"No2rCF_98xaD"},"source":["### Mandatory Run Parameters for Pipeline"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"a4DO6oms8xaE","executionInfo":{"status":"ok","timestamp":1653662238862,"user_tz":240,"elapsed":23,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["demo_run = True #Leave true to run the local demo dataset (without specifying any datapaths), make False to specify a different data folder path below\n","\n","#Target dataset folder path(must include one or more .txt or .csv datasets)\n","data_path = \"C:/Users/ryanu/OneDrive/Documents/GitHub/STREAMLINE/DemoData\" # (str) Demontration Data Path Folder\n","\n","#Output foder path: where to save pipeline outputs (must be updated for a given user)\n","output_path = 'C:/Users/ryanu/Documents/Analysis/STREAMLINE_Experiments' # (str) Demonstration Ouput Path Folder\n","\n","#Unique experiment name - folder created for this analysis within output folder path\n","experiment_name = 'hcc_demo'  # (str) Demontration Experiment Name\n","\n","# Data Labels\n","class_label = 'Class' # (str) i.e. class outcome column label\n","instance_label = 'InstanceID' # (str) If data includes instance labels, given respective column name here, otherwise put 'None'\n","\n","#Option to manually specify feature names to leave out of analysis, or which to treat as categorical (without using built in variable type detector)\n","ignore_features = [] # list of column names (given as string values) to exclude from the analysis (only insert column names if needed, otherwise leave empty)\n","categorical_feature_headers = [] # empty list for 'auto-detect' otherwise list feature names (given as string values) to be treated as categorical. Only impacts algorithms that can take variable type into account."]},{"cell_type":"markdown","metadata":{"id":"2siW3b428xaF"},"source":["### Run Parameters for Phase 1: Exploratory Analysis"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3pw3SbrL8xaF","executionInfo":{"status":"ok","timestamp":1653662238863,"user_tz":240,"elapsed":23,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["cv_partitions = 3  # (int, > 1) Number of training/testing data partitions to create - and resulting number of models generated using each ML algorithm\n","partition_method = 'S' # (str, S R or M) for stratified, random, or matched, respectively\n","match_label = 'None' # (str) Only applies when M selected for partition-method; indicates column label with matched instance ids' \n","\n","categorical_cutoff = 10 # (int) Bumber of unique values after which a variable is considered to be quantitative vs categorical\n","sig_cutoff = 0.05 # (float, 0-1) Significance cutoff used throughout pipeline\n","export_feature_correlations = 'True' # (str, True or False) Run and export feature correlation analysis (yields correlation heatmap)\n","export_univariate_plots = 'True' # (str, True or False) Export univariate analysis plots (note: univariate analysis still output by default)\n","topFeatures = 20 # (int) Number of top features to report in notebook for univariate analysis\n","random_state = 42 # (int) Sets a specific random seed for reproducible results"]},{"cell_type":"markdown","metadata":{"id":"TlBGrbIP8xaG"},"source":["### Run Parameters for Phase 2: Data Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Ynvy9oq08xaG","executionInfo":{"status":"ok","timestamp":1653662238864,"user_tz":240,"elapsed":22,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["scale_data = 'True' # (str, True or False) Perform data scaling?\n","impute_data = 'True' # (str, True or False) Perform missing value data imputation? (required for most ML algorithms if missing data is present)\n","overwrite_cv = 'True' # (str, True or False) Overwrites earlier cv datasets with new scaled/imputed ones\n","multi_impute = 'True' # (str, True or False) Applies multivariate imputation to quantitative features, otherwise uses mean imputation"]},{"cell_type":"markdown","metadata":{"id":"YNJfZ3UG8xaG"},"source":["### Run Parameters for Phase 3: Feature Importance Evaluation"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zGZ687yX8xaH","executionInfo":{"status":"ok","timestamp":1653662238864,"user_tz":240,"elapsed":20,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["do_mutual_info = 'True' # (str, True or False) Do mutual information analysis\n","do_multisurf = 'True' # (str, True or False) Do multiSURF analysis\n","use_TURF = 'False' # (str, True or False) Use TURF wrapper around MultiSURF\n","TURF_pct = 0.5 # (float, 0.01-0.5) Proportion of instances removed in an iteration (also dictates number of iterations)\n","njobs = -1 # (int) Number of cores dedicated to running algorithm; setting to -1 will use all available cores\n","instance_subset = 2000 # (int) Sample subset size to use with multiSURF"]},{"cell_type":"markdown","metadata":{"id":"WmiHkas28xaH"},"source":["### Run Parameters for Phase 4: Feature Selection"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"riKnVlcV8xaH","executionInfo":{"status":"ok","timestamp":1653662238865,"user_tz":240,"elapsed":19,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["max_features_to_keep = 2000 # (int) Maximum features to keep. 'None' if no max\n","filter_poor_features = 'True' # (str, True or False) Filter out the worst performing features prior to modeling\n","top_features = 40 # (int) Number of top features to illustrate in figures\n","export_scores = 'True' # (str, True or False) Export figure summarizing average feature importance scores over cv partitions"]},{"cell_type":"markdown","metadata":{"id":"L3M6YIBz8xaH"},"source":["### Run Parameters for Phase 5: Modeling"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"cE5okTK98xaI","executionInfo":{"status":"ok","timestamp":1653662384795,"user_tz":240,"elapsed":276,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#ML Model Algorithm Options (individual hyperparameter options can be adjusted below)\n","do_all = 'False'      # (str, True or False) indicates default value for whether all or none of the algorithms should be run\n","do_NB = 'True'        # (str, True or False, or None) Run naive bayes modeling\n","do_LR = 'True'        # (str, True or False, or None) Run logistic regression modeling\n","do_DT = 'True'        # (str, True or False, or None) Run decision tree modeling\n","do_RF = 'None'        # (str, True or False, or None) Run random forest modeling\n","do_GB = 'None'        # (str, True or False, or None) Run gradient boosting modeling\n","do_XGB = 'None'       # (str, True or False, or None) Run XGBoost modeling\n","do_LGB = 'None'       # (str, True or False, or None) Run LGBoost modeling\n","do_CGB = 'None'       # (str, True or False, or None) Run Catboost modeling\n","do_SVM = 'None'       # (str, True or False, or None) Run support vector machine modeling\n","do_ANN = 'None'       # (str, True or False, or None) Run artificial neural network modeling\n","do_KNN = 'None'       # (str, True or False, or None) Run k-neighbors classifier modeling\n","do_GP = 'None'        # (str, True or False, or None) Run genetic programming symbolic classifier modeling\n","\n","# ML Algorithms implemented by our reserach group: Rule-based ML Algorithm Options (Computationally expensive, so can be impractical to run hyperparameter sweep)\n","do_eLCS = 'False'     # (str, True or False, or None) Run eLCS modeling (a basic supervised-learning learning classifier system)\n","do_XCS = 'False'      # (str, True or False, or None) Run XCS modeling (a supervised-learning-only implementation of the best studied learning classifier system)\n","do_ExSTraCS = 'None' # (str, True or False, or None) Run ExSTraCS modeling (a learning classifier system designed for biomedical data mining)\n","\n","#Other Analysis Parameters\n","training_subsample = 0  # (int) For long running algorithms, option to subsample training set (0 for no subsample) Limit Sample Size Used to train algorithms that do not scale up well in large instance spaces (i.e. XGB,SVM,KN,ANN,and LR to a lesser degree) and depending on 'instances' settings, ExSTraCS, eLCS, and XCS)\n","use_uniform_FI = 'True' # (str, True or False) Overides use of any available feature importances estimate methods from models, instead using permutation_importance uniformly\n","primary_metric = 'balanced_accuracy' # (str) Must be an available metric identifier from (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n","\n","#Hyperparameter Sweep Options\n","n_trials = 200   # (int or None) Number of bayesian hyperparameter optimization trials using optuna\n","timeout = 900    # (int or None) Seconds until hyperparameter sweep stops running new trials (Note: it may run longer to finish last trial started)\n","export_hyper_sweep_plots = 'True' # (str, True or False) Export hyper parameter sweep plots from optuna\n","\n","#Learning classifier system specific options (ExSTraCS, eLCS, XCS)\n","do_lcs_sweep = 'False' # (str, True or False) Do LCS hyperparam tuning or use below params\n","nu = 1                 # (int, 0-10) Fixed LCS nu param\n","iterations = 200000    # (int, > data sample size) Fixed LCS # learning iterations param\n","N = 2000               # (int) > 500) Fixed LCS rule population maximum size param\n","lcs_timeout = 1200     # (int) Seconds until hyperparameter sweep stops for LCS algorithms (evolutionary algorithms often require more time for a single run)"]},{"cell_type":"markdown","metadata":{"id":"X49mNKtk8xaI"},"source":["### Hyperparameter Sweep Options for ML Algorithms\n","Users can extend or limit the range or options for given ML algorithm hyperparameters to be tested in hyperparameter optimization. These options are hardcoded when running this pipeline from the command line, but they are available here for users to see and modify. We have sought to include a broad range of relevant configurations based on online examples and relevant research publications. Use caution when modifying values below as improper modifications will lead to pipeline errors/failure. Links to available hyperparameter options for each algorithm are included below. "]},{"cell_type":"code","execution_count":16,"metadata":{"id":"XDTuVfkP8xaI","executionInfo":{"status":"ok","timestamp":1653662386418,"user_tz":240,"elapsed":269,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["def hyperparameters(random_state,do_lcs_sweep,nu,iterations,N,feature_names):\n","    param_grid = {}\n","    # Naive Bayes - no hyperparameters\n","    \n","    # Logistic Regression (Note: can take longer to run in data with larger instance spaces)\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n","    param_grid_LR = {'penalty': ['l2', 'l1'],'C': [1e-5, 1e5],'dual': [True, False],\n","                     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n","                     'class_weight': [None, 'balanced'],'max_iter': [10, 1000],\n","                     'random_state':[random_state]}\n","    \n","    # Decision Tree\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree%20classifier#sklearn.tree.DecisionTreeClassifier\n","    param_grid_DT = {'criterion': ['gini', 'entropy'],'splitter': ['best', 'random'],'max_depth': [1, 30],\n","                     'min_samples_split': [2, 50],'min_samples_leaf': [1, 50],'max_features': [None, 'auto', 'log2'],\n","                     'class_weight': [None, 'balanced'],\n","                     'random_state':[random_state]}\n","    \n","    # Random Forest\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier\n","    param_grid_RF = {'n_estimators': [10, 1000],'criterion': ['gini', 'entropy'],'max_depth': [1, 30],\n","                     'min_samples_split': [2, 50],'min_samples_leaf': [1, 50],'max_features': [None, 'auto', 'log2'],\n","                     'bootstrap': [True],'oob_score': [False, True],'class_weight': [None, 'balanced'],\n","                     'random_state':[random_state]}\n","    \n","    # Gradient Boosting Trees\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html?highlight=gradient%20boosting#sklearn.ensemble.GradientBoostingClassifier\n","    param_grid_GB = {'n_estimators': [10, 1000],'loss': ['deviance', 'exponential'], 'learning_rate': [.0001, 0.3], \n","                     'min_samples_leaf': [1, 50],'min_samples_split': [2, 50], 'max_depth': [1, 30],\n","                     'random_state':[random_state]}\n","    \n","    # XG Boost (Note: Not great for large instance spaces (limited completion) and class weight balance is included as option internally\n","    # https://xgboost.readthedocs.io/en/latest/parameter.html\n","    param_grid_XGB = {'booster': ['gbtree'],'objective': ['binary:logistic'],'verbosity': [0],'reg_lambda': [1e-8, 1.0],\n","                      'alpha': [1e-8, 1.0],'eta': [1e-8, 1.0],'gamma': [1e-8, 1.0],'max_depth': [1, 30],\n","                      'grow_policy': ['depthwise', 'lossguide'],'n_estimators': [10, 1000],'min_samples_split': [2, 50],\n","                      'min_samples_leaf': [1, 50],'subsample': [0.5, 1.0],'min_child_weight': [0.1, 10],\n","                      'colsample_bytree': [0.1, 1.0],'nthread':[1],'seed':[random_state]}\n","\n","    # LG Boost (Note: class weight balance is included as option internally (still takes a while on large instance spaces))\n","    # https://lightgbm.readthedocs.io/en/latest/Parameters.html\n","    param_grid_LGB = {'objective': ['binary'],'metric': ['binary_logloss'],'verbosity': [-1],'boosting_type': ['gbdt'],\n","                      'num_leaves': [2, 256],'max_depth': [1, 30],'lambda_l1': [1e-8, 10.0],'lambda_l2': [1e-8, 10.0],\n","                      'feature_fraction': [0.4, 1.0],'bagging_fraction': [0.4, 1.0],'bagging_freq': [1, 7],\n","                      'min_child_samples': [5, 100],'n_estimators': [10, 1000],'num_threads':[1],'seed':[random_state]}\n","\n","    # CatBoost - (Note this is newly added, and further optimization to this configuration is possible)\n","    # https://catboost.ai/en/docs/references/training-parameters/\n","    param_grid_CGB = {'learning_rate':[.0001, 0.3],'iterations':[10,500],'depth':[1,10],'l2_leaf_reg': [1,9],\n","                      'loss_function': ['Logloss'], 'random_seed': [random_state]}\n","    \n","    # Support Vector Machine (Note: Very slow in large instance spaces)\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n","    param_grid_SVM = {'kernel': ['linear', 'poly', 'rbf'],'C': [0.1, 1000],'gamma': ['scale'],'degree': [1, 6],\n","                      'probability': [True],'class_weight': [None, 'balanced'],'random_state':[random_state]}\n","    \n","    # Artificial Neural Network (Note: Slow in large instances spaces, and poor performer in small instance spaces)\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=artificial%20neural%20network\n","    param_grid_ANN = {'n_layers': [1, 3],'layer_size': [1, 100],'activation': ['identity', 'logistic', 'tanh', 'relu'],\n","                      'learning_rate': ['constant', 'invscaling', 'adaptive'],'momentum': [.1, .9],\n","                      'solver': ['sgd', 'adam'],'batch_size': ['auto'],'alpha': [0.0001, 0.05],'max_iter': [200],\n","                      'random_state':[random_state]}\n","    \n","    # K-Nearest Neighbor Classifier (Note: Runs slowly in data with large instance space)\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html?highlight=kneighborsclassifier#sklearn.neighbors.KNeighborsClassifier\n","    param_grid_KNN = {'n_neighbors': [1, 100], 'weights': ['uniform', 'distance'], 'p': [1, 5],\n","                     'metric': ['euclidean', 'minkowski']}\n","    \n","    # Genetic Programming Symbolic Classifier\n","    # https://gplearn.readthedocs.io/en/stable/reference.html\n","    param_grid_GP = {'population_size': [100, 1000], 'generations': [10, 500], 'tournament_size': [3, 50],'init_method': ['grow', 'full','half and half'],\n","                     'function_set': [['add', 'sub', 'mul', 'div'], ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv', 'max', 'min'], ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'abs', 'neg', 'inv', 'max', 'min','sin','cos','tan']],\n","                     'parsimony_coefficient': [0.001,0.01],'feature_names': [feature_names], 'low_memory': [True],'random_state': [random_state]}\n","\n","    # Learning Classifier Systems (i.e. eLCS, XCS, and ExSTraCS)\n","    # https://github.com/UrbsLab/scikit-eLCS\n","    # https://github.com/UrbsLab/scikit-XCS\n","    # https://github.com/UrbsLab/scikit-ExSTraCS\n","    \n","    if eval(do_lcs_sweep):\n","        # eLCS\n","        param_grid_eLCS = {'learning_iterations': [100000,200000,500000],'N': [1000,2000,5000],'nu': [1,10],\n","                           'random_state':[random_state]}\n","        # XCS\n","        param_grid_XCS = {'learning_iterations': [100000,200000,500000],'N': [1000,2000,5000],'nu': [1,10],\n","                          'random_state':[random_state]}\n","        # ExSTraCS\n","        param_grid_ExSTraCS = {'learning_iterations': [100000,200000,500000],'N': [1000,2000,5000],'nu': [1,10],\n","                               'random_state':[random_state],'rule_compaction':[None]}\n","    else:\n","        # eLCS\n","        param_grid_eLCS = {'learning_iterations': [iterations], 'N': [N], 'nu': [nu], 'random_state': [random_state]}\n","        # XCS\n","        param_grid_XCS = {'learning_iterations': [iterations], 'N': [N], 'nu': [nu], 'random_state': [random_state]}\n","        # ExSTraCS\n","        param_grid_ExSTraCS = {'learning_iterations': [iterations], 'N': [N], 'nu': [nu], 'random_state': [random_state], \n","                               'rule_compaction': ['QRF']} # 'None','QRF' - which is quick rule filter\n","        \n","    #Leave code below as is...\n","    param_grid['Naive Bayes'] = {}\n","    param_grid['Logistic Regression'] = param_grid_LR\n","    param_grid['Decision Tree'] = param_grid_DT\n","    param_grid['Random Forest'] = param_grid_RF\n","    param_grid['Gradient Boosting'] = param_grid_GB\n","    param_grid['Extreme Gradient Boosting'] = param_grid_XGB\n","    param_grid['Light Gradient Boosting'] = param_grid_LGB\n","    param_grid['Category Gradient Boosting'] = param_grid_CGB\n","    param_grid['Support Vector Machine'] = param_grid_SVM\n","    param_grid['Artificial Neural Network'] = param_grid_ANN\n","    param_grid['K-Nearest Neightbors'] = param_grid_KNN\n","    param_grid['Genetic Programming'] = param_grid_GP\n","    param_grid['eLCS'] = param_grid_eLCS\n","    param_grid['XCS'] = param_grid_XCS\n","    param_grid['ExSTraCS'] = param_grid_ExSTraCS\n","    return param_grid"]},{"cell_type":"markdown","metadata":{"id":"qAkK5s5T8xaJ"},"source":["### Run Parameters for Phase 6:  Statistics Summary and Figure Generation"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"UG1wx5j28xaJ","executionInfo":{"status":"ok","timestamp":1653662387225,"user_tz":240,"elapsed":3,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["plot_ROC = 'True'    # (str, True or False) Plot ROC curves individually for each algorithm including all CV results and averages\n","plot_PRC = 'True'    # (str, True or False) Plot PRC curves individually for each algorithm including all CV results and averages\n","plot_FI_box = 'True' # (str, True or False) Plot box plot summaries comparing algorithms for each metric\n","plot_metric_boxplots = 'True' # (str, True or False) Plot feature importance boxplots for each algorithm\n","metric_weight = 'balanced_accuracy' # (str, balanced_accuracy or roc_auc) ML model metric used as weight in composite FI plots (only supports balanced_accuracy or roc_auc as options) Recommend setting the same as primary_metric if possible.\n","top_model_features = 40  # (int) Number of top features in model to illustrate in figures"]},{"cell_type":"markdown","metadata":{"id":"XEnPw8qJ8xaJ"},"source":["### Run Parameters for Phase 10:  Apply Models to Replication Dataset\n","An optional phase to apply all trained models from previous phases to a separate 'replication' dataset which will be used to evaluate models across all algorithms and CV splits. In this demo, we didn't have a separate replication dataset to use for the UCI HCC dataset evaluated. Thus here we use a copy of the original HCC dataset as a 'pretend' replication dataset to demonstrate functionality. The replication data folder can include 1 or more datasets that can be evaluated as separate replication data. The user also needs to "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOIkDQ_K8xaJ"},"outputs":[],"source":["applyToReplication = True # (Boolean, True or False) Leave false unless you have a replication dataset handy to further evaluate/compare all models in uniform manner\n","rep_data_path = \"C:/Users/ryanu/OneDrive/Documents/GitHub/STREAMLINE/DemoRepData\" # (txt) Name of folder with replication Dataset(s)\n","dataset_for_rep = \"C:/Users/ryanu/OneDrive/Documents/GitHub/STREAMLINE/DemoData/hcc-data_example.csv\" # (txt) Path and name of dataset used to generate the models we want to apply (not the replication dataset)"]},{"cell_type":"markdown","metadata":{"id":"981KweE58xaK"},"source":["### Run Parameters for Phase 11:  File Cleanup\n","An optional phase to delete all unnecessary/temporary files generated by the pipeline."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"X3HL4aqa8xaK","executionInfo":{"status":"ok","timestamp":1653662239212,"user_tz":240,"elapsed":15,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["del_time = 'True'  # (str, True or False) Delete individual run-time files (but save summary)\n","del_oldCV = 'True' # (str, True or False) Delete any of the older versions of CV training and testing datasets not overwritten (preserves final training and testing datasets)"]},{"cell_type":"markdown","metadata":{"id":"leGT4A_x8xaK"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 1: Exploratory Analysis"]},{"cell_type":"markdown","metadata":{"id":"fcdQFMwe8xaK"},"source":["### Identify Working Directory"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"AEYXmgOX8xaK","executionInfo":{"status":"ok","timestamp":1653662239214,"user_tz":240,"elapsed":15,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["wd_path = os.getcwd() #Working directory path automatically detected\n","wd_path = wd_path.replace('\\\\','/')\n","sys.path.insert(1, wd_path+'/streamline')"]},{"cell_type":"markdown","metadata":{"id":"UhKtsBvW8xaK"},"source":["### Import Python Packages"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"CjJU5F_J8xaK","executionInfo":{"status":"error","timestamp":1653662378513,"user_tz":240,"elapsed":309,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}},"outputId":"3f0e8a79-7a0d-433f-8684-ad5721401e61"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-e1f9cd561390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mExploratoryAnalysisMain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExploratoryAnalysisJob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ExploratoryAnalysisMain'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import glob\n","import time\n","import csv\n","import pandas as pd\n","import numpy as np\n","import random\n","import pickle\n","import ExploratoryAnalysisMain\n","import ExploratoryAnalysisJob"]},{"cell_type":"markdown","metadata":{"id":"5WaVgabZ8xaL"},"source":["### Demo Setup\n","Bypasses whatever user may have entered into 'data_path' variable to ensure proper loading of local 'demo' dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKePd4VX8xaL","executionInfo":{"status":"aborted","timestamp":1653662239950,"user_tz":240,"elapsed":41,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if demo_run:\n","    data_path = wd_path+'/DemoData'\n","print(\"Data Folder Path: \"+data_path)\n","jupyterRun = 'True' #Leave True or pipeline will not display text or figures"]},{"cell_type":"markdown","metadata":{"id":"SSClYAAB8xaL"},"source":["### Run Exploratory Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCJFioK88xaL","executionInfo":{"status":"aborted","timestamp":1653662239951,"user_tz":240,"elapsed":41,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["ExploratoryAnalysisMain.makeDirTree(data_path,output_path,experiment_name,jupyterRun)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6IF0uaE8xaM","executionInfo":{"status":"aborted","timestamp":1653662239952,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Determine file extension of datasets in target folder:\n","file_count = 0\n","unique_datanames = []\n","for dataset_path in glob.glob(data_path+'/*'):\n","    dataset_path = str(dataset_path).replace('\\\\','/')\n","    print('---------------------------------------------------------------------------------')\n","    print(dataset_path)\n","    file_extension = dataset_path.split('/')[-1].split('.')[-1]\n","    data_name = dataset_path.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n","    if file_extension == 'txt' or file_extension == 'csv':\n","        if data_name not in unique_datanames:\n","            unique_datanames.append(data_name)\n","            ExploratoryAnalysisJob.runExplore(dataset_path,output_path+'/'+experiment_name,cv_partitions,partition_method,categorical_cutoff,export_feature_correlations,export_univariate_plots,class_label,instance_label,match_label,random_state,ignore_features,categorical_feature_headers,sig_cutoff,jupyterRun)\n","            file_count += 1\n","\n","if file_count == 0: #Check that there was at least 1 dataset\n","    raise Exception(\"There must be at least one .txt or .csv dataset in data_path directory\")\n","\n","#Create metadata dictionary object to keep track of pipeline run paramaters throughout phases\n","metadata = {}\n","metadata['Data Path'] = data_path\n","metadata['Output Path'] = output_path\n","metadata['Experiment Name'] = experiment_name\n","metadata['Class Label'] = class_label\n","metadata['Instance Label'] = instance_label\n","metadata['Ignored Features'] = ignore_features\n","metadata['Specified Categorical Features'] = categorical_feature_headers\n","metadata['CV Partitions'] = cv_partitions\n","metadata['Partition Method'] = partition_method\n","metadata['Match Label'] = match_label\n","metadata['Categorical Cutoff'] = categorical_cutoff\n","metadata['Statistical Significance Cutoff'] = sig_cutoff\n","metadata['Export Feature Correlations'] = export_feature_correlations\n","metadata['Export Univariate Plots'] = export_univariate_plots\n","metadata['Random Seed'] = random_state\n","metadata['Run From Jupyter Notebook'] = jupyterRun\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"1nvn01jZ8xaM"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 2: Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"eQ2l1jT98xaM"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCYji5aP8xaM","executionInfo":{"status":"aborted","timestamp":1653662239953,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import DataPreprocessingJob"]},{"cell_type":"markdown","metadata":{"id":"H_7upFsr8xaM"},"source":["### Run Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydsKAdWo8xaM","executionInfo":{"status":"aborted","timestamp":1653662239953,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path+\"/\"+experiment_name)\n","dataset_paths.remove('metadata.pickle')\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path+\"/\"+experiment_name+\"/\"+dataset_directory_path\n","    for cv_train_path in glob.glob(full_path+\"/CVDatasets/*Train.csv\"):\n","        cv_train_path = str(cv_train_path).replace('\\\\','/')\n","        cv_test_path = cv_train_path.replace(\"Train.csv\",\"Test.csv\")\n","        DataPreprocessingJob.job(cv_train_path,cv_test_path,output_path+'/'+experiment_name,scale_data,impute_data,overwrite_cv,categorical_cutoff,class_label,instance_label,random_state,multi_impute,jupyterRun)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file) \n","file.close()\n","    \n","#Update metadata\n","metadata['Use Data Scaling'] = scale_data\n","metadata['Use Data Imputation'] = impute_data\n","metadata['Use Multivariate Imputation'] = multi_impute\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"Iu3x4PyI8xaN"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 3: Feature Importance Evaluation"]},{"cell_type":"markdown","metadata":{"id":"Y_e1FVpq8xaN"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i9XDmqH88xaN","executionInfo":{"status":"aborted","timestamp":1653662239954,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import FeatureImportanceJob"]},{"cell_type":"markdown","metadata":{"id":"i1t8fqq48xaN"},"source":["### Run Feature Importance Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMqWcMpW8xaN","executionInfo":{"status":"aborted","timestamp":1653662239955,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path+\"/\"+experiment_name)\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path+\"/\"+experiment_name+\"/\"+dataset_directory_path\n","    experiment_path = output_path+'/'+experiment_name\n","\n","    if eval(do_mutual_info) or eval(do_multisurf):\n","        if not os.path.exists(full_path+\"/feature_selection\"):\n","            os.mkdir(full_path+\"/feature_selection\")\n","            \n","    if eval(do_mutual_info):\n","        if not os.path.exists(full_path+\"/feature_selection/mutualinformation\"):\n","            os.mkdir(full_path+\"/feature_selection/mutualinformation\")\n","        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*_CV_*Train.csv\"):\n","            cv_train_path = str(cv_train_path).replace('\\\\','/')\n","            FeatureImportanceJob.job(cv_train_path,experiment_path,random_state,class_label,instance_label,instance_subset,'mi',njobs,use_TURF,TURF_pct,jupyterRun)\n","\n","    if eval(do_multisurf):\n","        if not os.path.exists(full_path+\"/feature_selection/multisurf\"):\n","            os.mkdir(full_path+\"/feature_selection/multisurf\")\n","        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*_CV_*Train.csv\"):\n","            cv_train_path = str(cv_train_path).replace('\\\\','/')\n","            FeatureImportanceJob.job(cv_train_path,experiment_path,random_state,class_label,instance_label,instance_subset,'ms',njobs,use_TURF,TURF_pct,jupyterRun)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file) \n","file.close()\n","\n","#Update metadata\n","metadata['Use Mutual Information'] = do_mutual_info\n","metadata['Use MultiSURF'] = do_multisurf\n","metadata['Use TURF'] = use_TURF\n","metadata['TURF Cutoff'] = TURF_pct\n","metadata['MultiSURF Instance Subset'] = instance_subset\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"qwjc8_YD8xaN"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 4: Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"ABegQo1r8xaN"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRW0vIbb8xaO","executionInfo":{"status":"aborted","timestamp":1653662239955,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import FeatureSelectionJob"]},{"cell_type":"markdown","metadata":{"id":"iQKhDv258xaO"},"source":["### Run Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Waetd6Pf8xaO","executionInfo":{"status":"aborted","timestamp":1653662239956,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n","    FeatureSelectionJob.job(full_path,do_mutual_info,do_multisurf,max_features_to_keep,filter_poor_features,top_features,export_scores,class_label,instance_label,cv_partitions,overwrite_cv,jupyterRun)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","\n","#Update metadata\n","metadata['Max Features to Keep'] = max_features_to_keep\n","metadata['Filter Poor Features'] = filter_poor_features\n","metadata['Top Features to Display'] = top_features\n","metadata['Export Feature Importance Plot'] = export_scores\n","metadata['Overwrite CV Datasets'] = overwrite_cv\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"ADjdQ0LG8xaO"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 5: ML Modeling"]},{"cell_type":"markdown","metadata":{"id":"OB0mBxkC8xaO"},"source":["### Phase 5 Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oua_YXCn8xaO","executionInfo":{"status":"aborted","timestamp":1653662239957,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import ModelJob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfAh-fJY8xaO","executionInfo":{"status":"aborted","timestamp":1653662239958,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Create ML modeling algorithm information dictionary, given as ['algorithm used (set to true initially by default)','algorithm abreviation', 'color used for algorithm on figures']\n","### Note that other named colors used by matplotlib can be found here: https://matplotlib.org/3.5.0/_images/sphx_glr_named_colors_003.png\n","### Make sure new ML algorithm abbreviations and color designations are unique\n","algInfo = {}\n","algInfo['Naive Bayes'] = [True,'NB','silver']\n","algInfo['Logistic Regression'] = [True,'LR','dimgrey']\n","algInfo['Decision Tree'] = [True,'DT','yellow']\n","algInfo['Random Forest'] = [True,'RF','blue']\n","algInfo['Gradient Boosting'] = [True,'GB','cornflowerblue']\n","algInfo['Extreme Gradient Boosting'] = [True,'XGB','cyan']\n","algInfo['Light Gradient Boosting'] = [True,'LGB','pink']\n","algInfo['Category Gradient Boosting'] = [True,'CGB','magenta']\n","algInfo['Support Vector Machine'] = [True,'SVM','orange']\n","algInfo['Artificial Neural Network'] = [True,'ANN','red']\n","algInfo['K-Nearest Neightbors'] = [True,'KNN','chocolate']\n","algInfo['Genetic Programming'] = [True,'GP','purple']\n","algInfo['eLCS'] = [True,'eLCS','green']\n","algInfo['XCS'] = [True,'XCS','olive']\n","algInfo['ExSTraCS'] = [True,'ExSTraCS','lawngreen']\n","### Add new algorithms here...\n","\n","#Set up ML algorithm True/False use\n","if not eval(do_all): #If do all algorithms is false\n","    for key in algInfo:\n","        algInfo[key][0] = False #Set algorithm use to False\n","\n","#Set algorithm use truth for each algorithm specified by user (i.e. if user specified True/False for a specific algorithm)\n","if not do_NB == 'None':\n","    algInfo['Naive Bayes'][0] = eval(do_NB)\n","if not do_LR == 'None':\n","    algInfo['Logistic Regression'][0] = eval(do_LR)\n","if not do_DT == 'None':\n","    algInfo['Decision Tree'][0] = eval(do_DT)\n","if not do_RF == 'None':\n","    algInfo['Random Forest'][0] = eval(do_RF)\n","if not do_GB == 'None':\n","    algInfo['Gradient Boosting'][0] = eval(do_GB)\n","if not do_XGB == 'None':\n","    algInfo['Extreme Gradient Boosting'][0] = eval(do_XGB)\n","if not do_LGB == 'None':\n","    algInfo['Light Gradient Boosting'][0] = eval(do_LGB)\n","if not do_CGB == 'None':\n","    algInfo['Category Gradient Boosting'][0] = eval(do_CGB)\n","if not do_SVM == 'None':\n","    algInfo['Support Vector Machine'][0] = eval(do_SVM)\n","if not do_ANN == 'None':\n","    algInfo['Artificial Neural Network'][0] = eval(do_ANN)\n","if not do_KNN == 'None':\n","    algInfo['K-Nearest Neightbors'][0] = eval(do_KNN)\n","if not do_GP == 'None':\n","    algInfo['Genetic Programming'][0] = eval(do_GP)\n","if not do_eLCS == 'None':\n","    algInfo['eLCS'][0] = eval(do_eLCS)\n","if not do_XCS == 'None':\n","    algInfo['XCS'][0] = eval(do_XCS)\n","if not do_ExSTraCS == 'None':\n","    algInfo['ExSTraCS'][0] = eval(do_ExSTraCS)\n","### Add new algorithms here...\n","\n","#Pickle the algorithm information dictionary for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"algInfo.pickle\", 'wb')\n","pickle.dump(algInfo,pickle_out)\n","pickle_out.close()\n","\n","#Make list of algorithms to be run (full names)\n","algorithms = []\n","for key in algInfo:\n","    if algInfo[key][0]: #Algorithm is true\n","        algorithms.append(key)"]},{"cell_type":"markdown","metadata":{"id":"U8ydTmg68xaP"},"source":["### Run ML Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wzjqahzE8xaP","executionInfo":{"status":"aborted","timestamp":1653662239958,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n","    if not os.path.exists(full_path+'/models'):\n","        os.mkdir(full_path+'/models')\n","    if not os.path.exists(full_path+'/model_evaluation'):\n","        os.mkdir(full_path+'/model_evaluation')\n","    if not os.path.exists(full_path+'/models/pickledModels'):\n","        os.mkdir(full_path+'/models/pickledModels')\n","\n","    for cvCount in range(cv_partitions):\n","        train_file_path = full_path+'/CVDatasets/'+dataset_directory_path+\"_CV_\"+str(cvCount)+\"_Train.csv\"\n","        test_file_path = full_path + '/CVDatasets/' + dataset_directory_path + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n","        for algorithm in algorithms:\n","            algAbrev = algInfo[algorithm][1]\n","            #Get header names for current CV dataset for use later in GP tree visulaization\n","            data_name = full_path.split('/')[-1]\n","            feature_names = pd.read_csv(full_path+'/CVDatasets/'+data_name+'_CV_'+str(cvCount)+'_Test.csv').columns.values.tolist()\n","            if instance_label != 'None':\n","                feature_names.remove(instance_label)\n","            feature_names.remove(class_label)\n","            #Get hyperparameter grid\n","            param_grid = hyperparameters(random_state,do_lcs_sweep,nu,iterations,N,feature_names)[algorithm]\n","            ModelJob.runModel(algorithm,train_file_path,test_file_path,full_path,n_trials,timeout,lcs_timeout,export_hyper_sweep_plots,instance_label,class_label,random_state,cvCount,filter_poor_features,do_lcs_sweep,nu,iterations,N,training_subsample,use_uniform_FI,primary_metric,param_grid,algAbrev)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file) \n","file.close()\n","\n","#Update metadata\n","metadata['Naive Bayes'] = str(algInfo['Naive Bayes'][0])\n","metadata['Logistic Regression'] = str(algInfo['Logistic Regression'][0])\n","metadata['Decision Tree'] = str(algInfo['Decision Tree'][0])\n","metadata['Random Forest'] = str(algInfo['Random Forest'][0])\n","metadata['Gradient Boosting'] = str(algInfo['Gradient Boosting'][0])\n","metadata['Extreme Gradient Boosting'] = str(algInfo['Extreme Gradient Boosting'][0])\n","metadata['Light Gradient Boosting'] = str(algInfo['Light Gradient Boosting'][0])\n","metadata['Category Gradient Boosting'] = str(algInfo['Category Gradient Boosting'][0])\n","metadata['Support Vector Machine'] = str(algInfo['Support Vector Machine'][0])\n","metadata['Artificial Neural Network'] = str(algInfo['Artificial Neural Network'][0])\n","metadata['K-Nearest Neightbors'] = str(algInfo['K-Nearest Neightbors'][0])\n","metadata['Genetic Programming'] = str(algInfo['Genetic Programming'][0])\n","metadata['eLCS'] = str(algInfo['eLCS'][0])\n","metadata['XCS'] = str(algInfo['XCS'][0])\n","metadata['ExSTraCS'] = str(algInfo['ExSTraCS'][0])\n","### Add new algorithms here...\n","metadata['Primary Metric'] = primary_metric\n","metadata['Training Subsample for KNN,ANN,SVM,and XGB'] = training_subsample\n","metadata['Uniform Feature Importance Estimation (Models)'] = use_uniform_FI\n","metadata['Hyperparameter Sweep Number of Trials'] = n_trials\n","metadata['Hyperparameter Sweep Number of Trials'] = n_trials\n","metadata['Hyperparameter Timeout'] = timeout\n","metadata['Export Hyperparameter Sweep Plots'] = export_hyper_sweep_plots\n","metadata['Do LCS Hyperparameter Sweep'] = do_lcs_sweep\n","metadata['LCS Hyperparameter: nu'] = nu\n","metadata['LCS Hyperparameter: Training Iterations'] = iterations\n","metadata['LCS Hyperparameter: N - Rule Population Size'] = N\n","metadata['LCS Hyperparameter Sweep Timeout'] = lcs_timeout\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"OKKbxRBb8xaP"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 6: Statistics (Stats Summaries, Figures, Statistical Comparisons)"]},{"cell_type":"markdown","metadata":{"id":"PHNusxIf8xaP"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWunPhcC8xaP","executionInfo":{"status":"aborted","timestamp":1653662239959,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import StatsJob"]},{"cell_type":"markdown","metadata":{"id":"vr2c5jw_8xaP"},"source":["### Run Statistics Summary and Figure Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"PGVxOnJq8xaQ","executionInfo":{"status":"aborted","timestamp":1653662239960,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","metadata['Export ROC Plot'] = plot_ROC\n","metadata['Export PRC Plot'] = plot_PRC\n","metadata['Export Metric Boxplots'] = plot_metric_boxplots\n","metadata['Export Feature Importance Boxplots'] = plot_FI_box\n","metadata['Metric Weighting Composite FI Plots'] = metric_weight\n","metadata['Top Model Features To Display'] = top_model_features\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()\n","\n","#Now that primary pipeline phases are complete generate a human readable version of metadata\n","df = pd.DataFrame.from_dict(metadata, orient ='index')\n","df.to_csv(output_path+'/'+experiment_name+'/'+'metadata.csv',index=True)\n","\n","# Iterate through datasets\n","dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n","    StatsJob.job(full_path,plot_ROC,plot_PRC,plot_FI_box,class_label,instance_label,cv_partitions,scale_data,plot_metric_boxplots,primary_metric,top_model_features,sig_cutoff,metric_weight,jupyterRun)"]},{"cell_type":"markdown","metadata":{"id":"QaPMenLE8xaQ"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 7: Dataset Comparison (Optional: Use only if > 1 dataset was analyzed)"]},{"cell_type":"markdown","metadata":{"id":"tbCxplsv8xaQ"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBC8j-wz8xaQ","executionInfo":{"status":"aborted","timestamp":1653662239960,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import DataCompareJob"]},{"cell_type":"markdown","metadata":{"id":"N-XCcE-b8xaQ"},"source":["### Run Dataset Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpzECAa88xaQ","executionInfo":{"status":"aborted","timestamp":1653662239961,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if len(dataset_paths) > 1:\n","    DataCompareJob.job(output_path+'/'+experiment_name,sig_cutoff,jupyterRun)"]},{"cell_type":"markdown","metadata":{"id":"L28xx5Tn8xaQ"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 8: PDF Training Report Generator (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og6MfjB98xaQ","executionInfo":{"status":"aborted","timestamp":1653662239962,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import PDF_ReportJob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dURrznaU8xaR","executionInfo":{"status":"aborted","timestamp":1653662239962,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["experiment_path = output_path+'/'+experiment_name\n","PDF_ReportJob.job(experiment_path,'True','None','None')"]},{"cell_type":"markdown","metadata":{"id":"9oO7fBSM8xaR"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 9: Apply Models to Replication Data (Optional)"]},{"cell_type":"markdown","metadata":{"id":"LBoXS8T08xaR"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7WGUV218xaR","executionInfo":{"status":"aborted","timestamp":1653662239963,"user_tz":240,"elapsed":43,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import ApplyModelJob"]},{"cell_type":"markdown","metadata":{"id":"quCCXyYp8xaR"},"source":["### Specify Run Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ettEkWVe8xaR","executionInfo":{"status":"aborted","timestamp":1653662239963,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if demo_run:\n","    rep_data_path = wd_path+'/DemoRepData'\n","    dataset_for_rep = wd_path+'/DemoData/hcc-data_example.csv'\n","print(\"Replication Data Folder Path: \"+rep_data_path)\n","print(\"Dataset Path: \"+dataset_for_rep)"]},{"cell_type":"markdown","metadata":{"id":"KIdOvjOW8xaR"},"source":["### Run Application of Models to Replication Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2xcZKir8xaR","executionInfo":{"status":"aborted","timestamp":1653662239964,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if applyToReplication:\n","    data_name = dataset_for_rep.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n","    full_path = output_path + \"/\" + experiment_name + \"/\" + data_name #location of folder containing models respective training dataset\n","\n","    if not os.path.exists(full_path+\"/applymodel\"):\n","        os.mkdir(full_path+\"/applymodel\")\n","\n","    #Determine file extension of datasets in target folder:\n","    file_count = 0\n","    unique_datanames = []\n","    for datasetFilename in glob.glob(rep_data_path+'/*'):\n","        datasetFilename = str(datasetFilename).replace('\\\\','/')\n","\n","        file_extension = datasetFilename.split('/')[-1].split('.')[-1]\n","        apply_name = datasetFilename.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n","        if not os.path.exists(full_path+\"/applymodel/\"+apply_name):\n","            os.mkdir(full_path+\"/applymodel/\"+apply_name)\n","\n","        if file_extension == 'txt' or file_extension == 'csv':\n","            if apply_name not in unique_datanames:\n","                unique_datanames.append(apply_name)\n","                ApplyModelJob.job(datasetFilename,full_path,class_label,instance_label,categorical_cutoff,sig_cutoff,cv_partitions,scale_data,impute_data,primary_metric,dataset_for_rep,match_label,plot_ROC,plot_PRC,plot_metric_boxplots,export_feature_correlations,jupyterRun,multi_impute)\n","                file_count += 1\n","\n","    if file_count == 0: #Check that there was at least 1 dataset\n","        raise Exception(\"There must be at least one .txt or .csv dataset in rep_data_path directory\")"]},{"cell_type":"markdown","metadata":{"id":"EHqUjpMw8xaS"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 10: PDF Apply Report Generator (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdVXOJ1V8xaS","executionInfo":{"status":"aborted","timestamp":1653662239965,"user_tz":240,"elapsed":42,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import PDF_ReportJob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3eeGNqD8xaS","executionInfo":{"status":"aborted","timestamp":1653662239965,"user_tz":240,"elapsed":41,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if applyToReplication:\n","    experiment_path = output_path+'/'+experiment_name\n","    PDF_ReportJob.job(experiment_path,'False',rep_data_path,dataset_for_rep)"]},{"cell_type":"markdown","metadata":{"id":"8PU10_hB8xaS"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 11: File Cleanup (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tIM4p_K8xaS","executionInfo":{"status":"aborted","timestamp":1653662240211,"user_tz":240,"elapsed":287,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import shutil"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wObz6bkh8xaS","executionInfo":{"status":"aborted","timestamp":1653662240212,"user_tz":240,"elapsed":287,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["# Get dataset paths for all completed dataset analyses in experiment folder\n","datasets = os.listdir(experiment_path)\n","experiment_name = experiment_path.split('/')[-1] #Name of experiment folder\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in datasets:\n","        datasets.remove(text)\n","\n","#Delete jobscompleted folder/files\n","try:\n","    shutil.rmtree(experiment_path+'/'+'jobsCompleted')\n","except:\n","    pass\n","\n","#Delete target files within each dataset subfolder\n","for dataset in datasets:\n","    #Delete individual runtime files (save runtime summary generated in phase 6)\n","    if eval(del_time):\n","        try:\n","            shutil.rmtree(experiment_path+'/'+dataset+'/'+'runtime')\n","            print(\"Individual Runtime Files Deleted\")\n","        except:\n","            pass\n","    #Delete temporary feature importance pickle files (only needed for phase 4 and then saved as summary files in phase 6)\n","    try:\n","        shutil.rmtree(experiment_path+'/'+dataset+'/feature_selection/mutualinformation/pickledForPhase4')\n","        print(\"Mutual Information Pickle Files Deleted\")\n","    except:\n","        pass\n","    try:\n","        shutil.rmtree(experiment_path+'/'+dataset+'/feature_selection/multisurf/pickledForPhase4')\n","        print(\"MultiSURF Pickle Files Deleted\")\n","    except:\n","        pass\n","    #Delete older training and testing CV datasets (does not delete any final versions used for training). Older cv datasets might have been kept to see what they look like prior to preprocessing and feature selection.\n","    if eval(del_oldCV):\n","        #Delete CV files generated after preprocessing but before feature selection\n","        files = glob.glob(experiment_path+'/'+dataset+'/CVDatasets/*CVOnly*')\n","        for f in files:\n","            try:\n","                os.remove(f)\n","                print(\"Deleted Intermediary CV-Only Dataset Files\")\n","            except:\n","                pass\n","        #Delete CV files generated after CV partitioning but before preprocessing\n","        files = glob.glob(experiment_path+'/'+dataset+'/CVDatasets/*CVPre*')\n","        for f in files:\n","            try:\n","                os.remove(f)\n","                print(\"Deleted Intermediary CV-Pre Dataset Files\")\n","            except:\n","                pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gtLQdMx8xaT","executionInfo":{"status":"aborted","timestamp":1653662240212,"user_tz":240,"elapsed":286,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"STREAMLINE-Notebook.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}