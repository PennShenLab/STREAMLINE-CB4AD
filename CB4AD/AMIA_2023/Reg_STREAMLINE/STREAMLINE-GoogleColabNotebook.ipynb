{"cells":[{"cell_type":"markdown","metadata":{"id":"h47eTFUk8dzY"},"source":["# Summary"]},{"cell_type":"markdown","metadata":{"id":"BZQpJZw-8dzb"},"source":["This notebook runs all aspects of the Regression STREAMLINE which is an automated machine learning analysis pipeline for regression tasks. Of note, two potentially important elements that are not automated by this pipeline include careful data cleaning and feature engineering using problem domain knowledge. Please review the README included in the associated GitHub repository for a detailed overview of how to run this pipeline. For simplicity, this notebook runs Python code outside of what is visible within it."]},{"cell_type":"markdown","metadata":{"id":"ProUmuu-8dzb"},"source":["## Google Collab and Run Enviornment Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"qW5WNjL28dzc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688079253970,"user_tz":240,"elapsed":29688,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}},"outputId":"96613e02-49e3-45e5-e1f8-c55cca5e47c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"SPjhJvYE8dzd","executionInfo":{"status":"ok","timestamp":1688079264615,"user_tz":240,"elapsed":10650,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Load all require local python files on from Google Drive\n","from google.colab import files\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/ExploratoryAnalysisMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/ExploratoryAnalysisJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/DataPreprocessingMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/DataPreprocessingJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/FeatureImportanceMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/FeatureImportanceJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/FeatureSelectionMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/FeatureSelectionJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/ModelMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/ModelJob.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/l21regjob.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/smogn.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/StatsMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/StatsJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/DataCompareMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/DataCompareJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/PDF_ReportMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/PDF_ReportJob.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/PDF_ReportJob_Reg.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/ApplyModelMain.py /content\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/ApplyModelJob.py /content\n","\n","!cp /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/streamline/FileCleanup.py /content"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUn9x39T8dze","executionInfo":{"status":"ok","timestamp":1688079361693,"user_tz":240,"elapsed":97093,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}},"outputId":"5c1e96a7-0fd0-42a4-b108-fc894ada72d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting skrebate==0.7\n","  Downloading skrebate-0.7.tar.gz (22 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skrebate==0.7) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from skrebate==0.7) (1.10.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from skrebate==0.7) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skrebate==0.7) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skrebate==0.7) (3.1.0)\n","Building wheels for collected packages: skrebate\n","  Building wheel for skrebate (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for skrebate: filename=skrebate-0.7-py3-none-any.whl size=30620 sha256=372ce64e664fd585d1b7ed04d85166468ef193effd43772213f324c0b192783d\n","  Stored in directory: /root/.cache/pip/wheels/3f/c9/40/5911f5f510e1252264e84984b560446d8e63241b709d82f7d5\n","Successfully built skrebate\n","Installing collected packages: skrebate\n","Successfully installed skrebate-0.7\n","Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (1.7.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.10.1)\n","Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (3.3.5)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm) (0.40.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.10.1)\n","Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.2.2)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n","Collecting catboost\n","  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.22.4)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.13.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2022.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.40.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (8.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.2)\n","Installing collected packages: catboost\n","Successfully installed catboost-1.2\n","Collecting gplearn\n","  Downloading gplearn-0.4.2-py3-none-any.whl (25 kB)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from gplearn) (1.2.2)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gplearn) (1.2.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.22.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->gplearn) (1.10.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->gplearn) (3.1.0)\n","Installing collected packages: gplearn\n","Successfully installed gplearn-0.4.2\n","Collecting scikit-eLCS\n","  Downloading scikit-eLCS-1.2.4.tar.gz (259 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-eLCS) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scikit-eLCS) (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from scikit-eLCS) (1.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scikit-eLCS) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scikit-eLCS) (2022.7.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-eLCS) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-eLCS) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-eLCS) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->scikit-eLCS) (1.16.0)\n","Building wheels for collected packages: scikit-eLCS\n","  Building wheel for scikit-eLCS (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-eLCS: filename=scikit_eLCS-1.2.4-py3-none-any.whl size=38958 sha256=da230fd78bfd42b58ebc6d240b9467d59f5ab83f441401f491a506f9cfe3b97c\n","  Stored in directory: /root/.cache/pip/wheels/0a/9d/e2/a17d6ce42dc869399bf148eeb8cfb231890b75f1dd2f63ac0c\n","Successfully built scikit-eLCS\n","Installing collected packages: scikit-eLCS\n","Successfully installed scikit-eLCS-1.2.4\n","Collecting scikit-XCS\n","  Downloading scikit-XCS-1.0.8.tar.gz (317 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.6/317.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-XCS) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scikit-XCS) (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from scikit-XCS) (1.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scikit-XCS) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scikit-XCS) (2022.7.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-XCS) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-XCS) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-XCS) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->scikit-XCS) (1.16.0)\n","Building wheels for collected packages: scikit-XCS\n","  Building wheel for scikit-XCS (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-XCS: filename=scikit_XCS-1.0.8-py3-none-any.whl size=36284 sha256=209e5f14f6d9445f5c97769a11f73f81a6c37a805d1bf4eb275f3aba47f78826\n","  Stored in directory: /root/.cache/pip/wheels/79/f4/4f/eee3d2542d5ea0419f97d2e762500cabb50f3aff91ed79084a\n","Successfully built scikit-XCS\n","Installing collected packages: scikit-XCS\n","Successfully installed scikit-XCS-1.0.8\n","Collecting scikit-ExSTraCS\n","  Downloading scikit-ExSTraCS-1.1.1.tar.gz (685 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m685.9/685.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from scikit-ExSTraCS) (1.22.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from scikit-ExSTraCS) (1.5.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from scikit-ExSTraCS) (1.2.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scikit-ExSTraCS) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->scikit-ExSTraCS) (2022.7.1)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-ExSTraCS) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-ExSTraCS) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->scikit-ExSTraCS) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->scikit-ExSTraCS) (1.16.0)\n","Building wheels for collected packages: scikit-ExSTraCS\n","  Building wheel for scikit-ExSTraCS (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for scikit-ExSTraCS: filename=scikit_ExSTraCS-1.1.1-py3-none-any.whl size=46573 sha256=54a8d1d68a48cf5bfce8f663dcb1871c39d0d2989b088416c1cb9313b551568e\n","  Stored in directory: /root/.cache/pip/wheels/fe/c0/3d/70d54e4f36936d980db2b92848f4f79ab159f840cf0a0bd9dc\n","Successfully built scikit-ExSTraCS\n","Installing collected packages: scikit-ExSTraCS\n","Successfully installed scikit-ExSTraCS-1.1.1\n","Collecting optuna==2.0.0\n","  Downloading optuna-2.0.0.tar.gz (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.9/226.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting alembic (from optuna==2.0.0)\n","  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cliff (from optuna==2.0.0)\n","  Downloading cliff-4.3.0-py3-none-any.whl (80 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmaes>=0.5.1 (from optuna==2.0.0)\n","  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n","Collecting colorlog (from optuna==2.0.0)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from optuna==2.0.0) (1.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna==2.0.0) (1.22.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from optuna==2.0.0) (23.1)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.10/dist-packages (from optuna==2.0.0) (1.10.1)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from optuna==2.0.0) (2.0.16)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna==2.0.0) (4.65.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.1.0->optuna==2.0.0) (4.6.3)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.1.0->optuna==2.0.0) (2.0.2)\n","Collecting Mako (from alembic->optuna==2.0.0)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from cliff->optuna==2.0.0) (0.7.2)\n","Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.10/dist-packages (from cliff->optuna==2.0.0) (6.0)\n","Collecting autopage>=0.4.0 (from cliff->optuna==2.0.0)\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting cmd2>=1.0.0 (from cliff->optuna==2.0.0)\n","  Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.2/147.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting importlib-metadata>=4.4 (from cliff->optuna==2.0.0)\n","  Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n","Collecting stevedore>=2.0.1 (from cliff->optuna==2.0.0)\n","  Downloading stevedore-5.1.0-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.10/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.0.0) (23.1.0)\n","Collecting pyperclip>=1.6 (from cmd2>=1.0.0->cliff->optuna==2.0.0)\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.0.0) (0.2.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.4->cliff->optuna==2.0.0) (3.15.0)\n","Collecting pbr!=2.1.0,>=2.0.0 (from stevedore>=2.0.1->cliff->optuna==2.0.0)\n","  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic->optuna==2.0.0) (2.1.3)\n","Building wheels for collected packages: optuna, pyperclip\n","  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for optuna: filename=optuna-2.0.0-py3-none-any.whl size=312821 sha256=b36a5395c026eb7cff2511e131dc10510537552788e1fea31777d609fb1b8bc3\n","  Stored in directory: /root/.cache/pip/wheels/88/08/60/552399d1ae00b0d56554e4c0f80809c4c42a94de19e68ceed3\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11124 sha256=72489f67f88aa7677ea608e1156b5678b7f983d16692701004f41f8350e3bef5\n","  Stored in directory: /root/.cache/pip/wheels/04/24/fe/140a94a7f1036003ede94579e6b4227fe96c840c6f4dcbe307\n","Successfully built optuna pyperclip\n","Installing collected packages: pyperclip, pbr, Mako, importlib-metadata, colorlog, cmd2, cmaes, autopage, stevedore, alembic, cliff, optuna\n","Successfully installed Mako-1.2.4 alembic-1.11.1 autopage-0.5.1 cliff-4.3.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 importlib-metadata-6.7.0 optuna-2.0.0 pbr-5.11.1 pyperclip-1.8.2 stevedore-5.1.0\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.13.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.2.2)\n","Collecting kaleido==0.0.3.post1\n","  Downloading kaleido-0.0.3.post1-py2.py3-none-manylinux1_x86_64.whl (74.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: kaleido\n","Successfully installed kaleido-0.0.3.post1\n","Collecting fpdf\n","  Downloading fpdf-1.7.2.tar.gz (39 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: fpdf\n","  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=04080dd9b59a683d191b62b1f76557bfcbe719f834ab5358a5d2cf61e7446aa2\n","  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n","Successfully built fpdf\n","Installing collected packages: fpdf\n","Successfully installed fpdf-1.7.2\n","Collecting group-lasso\n","  Downloading group_lasso-1.5.0-py3-none-any.whl (33 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from group-lasso) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from group-lasso) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->group-lasso) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->group-lasso) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->group-lasso) (3.1.0)\n","Installing collected packages: group-lasso\n","Successfully installed group-lasso-1.5.0\n"]}],"source":["#Install remaining required packages not preinstalled in Google Collab\n","!pip install skrebate==0.7\n","!pip install xgboost\n","!pip install lightgbm\n","!pip install catboost\n","!pip install gplearn\n","!pip install scikit-eLCS\n","!pip install scikit-XCS\n","!pip install scikit-ExSTraCS\n","!pip install optuna==2.0.0\n","!pip install plotly\n","!pip install kaleido==0.0.3.post1\n","!pip install fpdf\n","!pip install group-lasso"]},{"cell_type":"markdown","metadata":{"id":"HRu0j-Rg8dze"},"source":["## Notebook Housekeeping\n","Set up notebook cells to display desired results. No need to edit."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eSfXe3qO8dzf","executionInfo":{"status":"ok","timestamp":1688079361693,"user_tz":240,"elapsed":14,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import warnings\n","import sys\n","import os\n","import shutil\n","warnings.filterwarnings('ignore')\n","\n","# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last.\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""]},{"cell_type":"markdown","metadata":{"id":"laRm11sV8dzf"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## (User Specified) Run Parameters of STREAMLINE\n","These initial notebook cells include all customizable run parameters for STREAMLINE. These settings should only be left unchanged for users wishing to test out the pipeline demo (as is) to learn how it works or to confirm efficacy before running their own data. Run parameters for each phase of the pipeline are included in separate code cells of this section of the notebook.\n"]},{"cell_type":"markdown","metadata":{"id":"kGZtOKm38dzg"},"source":["### Mandatory Run Parameters for Pipeline"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"MjwoF3TA8dzg","executionInfo":{"status":"ok","timestamp":1688079361693,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["demo_run = False #Leave true to run the local demo dataset (without specifying any datapaths), make False to specify a different data folder path below\n","\n","#Target dataset folder path(must include one or more .txt or .csv datasets)\n","data_path = \"/content/drive/MyDrive/STREAMLINE-Regression/Measurements/cdrsb\" # (str) Demontration Data Path Folder\n","\n","#Output foder path: where to save pipeline outputs (must be updated for a given user)\n","output_path = '/content/drive/MyDrive/STREAMLINE-Regression/Colab_Output' # (str) Demonstration Ouput Path Folder\n","\n","#Unique experiment name - folder created for this analysis within output folder path\n","experiment_name = 'cdrsb_experiment'  # (str) Demontration Experiment Name\n","\n","# Data Labels\n","class_label = 'Cognition_Score' # (str) i.e. class outcome column label\n","instance_label = 'Class' # (str) If data includes instance labels, given respective column name here, otherwise put 'None'\n","\n","#Option to manually specify feature names to leave out of analysis, or which to treat as categorical (without using built in variable type detector)\n","ignore_features = [] # list of column names (given as string values) to exclude from the analysis (only insert column names if needed, otherwise leave empty)\n","categorical_feature_headers = [] # empty list for 'auto-detect' otherwise list feature names (given as string values) to be treated as categorical. Only impacts algorithms that can take variable type into account."]},{"cell_type":"markdown","metadata":{"id":"bQ_9fYJ88dzg"},"source":["### Run Parameters for Phase 1: Exploratory Analysis"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_xgab9CQ8dzh","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["cv_partitions = 5  # (int, > 1) Number of training/testing data partitions to create - and resulting number of models generated using each ML algorithm\n","partition_method = 'R' # (str, S R or M) for stratified, random, or matched, respectively\n","match_label = 'None' # (str) Only applies when M selected for partition-method; indicates column label with matched instance ids'\n","\n","categorical_cutoff = 10 # (int) Number of unique values after which a variable is considered to be quantitative vs categorical\n","sig_cutoff = 0.05 # (float, 0-1) Significance cutoff used throughout pipeline\n","export_feature_correlations = 'True' # (str, True or False) Run and export feature correlation analysis (yields correlation heatmap)\n","export_univariate_plots = 'False' # (str, True or False) Export univariate analysis plots (note: univariate analysis still output by default)\n","topFeatures = 20 # (int) Number of top features to report in notebook for univariate analysis\n","random_state = 42 # (int) Sets a specific random seed for reproducible results"]},{"cell_type":"markdown","metadata":{"id":"rAq3yrep8dzh"},"source":["### Run Parameters for Phase 2: Data Preprocessing"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tq4DzpKG8dzh","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["scale_data = 'True' # (str, True or False) Perform data scaling?\n","impute_data = 'True' # (str, True or False) Perform missing value data imputation? (required for most ML algorithms if missing data is present)\n","overwrite_cv = 'True' # (str, True or False) Overwrites earlier cv datasets with new scaled/imputed ones\n","multi_impute = 'True' # (str, True or False) Applies multivariate imputation to quantitative features, otherwise uses mean imputation"]},{"cell_type":"markdown","metadata":{"id":"OEyuMjCY8dzh"},"source":["### Run Parameters for Phase 3: Feature Importance Evaluation"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"3q5Scuj18dzi","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":11,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["do_mutual_info = 'True' # (str, True or False) Do mutual information analysis\n","do_multisurf = 'True' # (str, True or False) Do multiSURF analysis\n","use_TURF = 'False' # (str, True or False) Use TURF wrapper around MultiSURF\n","TURF_pct = 0.5 # (float, 0.01-0.5) Proportion of instances removed in an iteration (also dictates number of iterations)\n","njobs = -1 # (int) Number of cores dedicated to running algorithm; setting to -1 will use all available cores\n","instance_subset = 2000 # (int) Sample subset size to use with multiSURF"]},{"cell_type":"markdown","metadata":{"id":"7CdYOXsk8dzi"},"source":["### Run Parameters for Phase 4: Feature Selection"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"bo2JaxGl8dzi","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":11,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["max_features_to_keep = 2000 # (int) Maximum features to keep.\n","filter_poor_features = 'False' # (str, True or False) Filter out the worst performing features prior to modeling\n","top_features = 40 # (int) Number of top features to illustrate in figures\n","export_scores = 'True' # (str, True or False) Export figure summarizing average feature importance scores over cv partitions"]},{"cell_type":"markdown","metadata":{"id":"81YkNYlA8dzi"},"source":["### Run Parameters for Phase 5: Modeling"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"cIINLJio8dzi","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":11,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#ML Model Algorithm Options (individual hyperparameter options can be adjusted below)\n","do_all = 'False'\n","# Regression Algorithm\n","do_CommonReg = 'True'\n","do_linReg = 'True'\n","do_ENReg = 'True'\n","do_RFReg = 'False'\n","do_AdaReg = 'False'\n","do_GradReg = 'False'\n","do_SVR = 'True'\n","do_GL = 'False'\n","\n","# L21 Regression Series Algorithm\n","do_L21Series = 'True'\n","do_L21Reg = 'True'\n","do_L21GMMReg = 'False'\n","do_L21DGMMReg = 'False'\n","\n","#Group Lasso Parameters - Defaults available\n","groups_path = '/content/drive/MyDrive/STREAMLINE-Regression/streamline/groups.csv' # (str) Path of the defined groups\n","\n","# Other Analysis Parameters\n","training_subsample = 0  # (int) For long running algorithms, option to subsample training set (0 for no subsample) Limit Sample Size Used to train algorithms that do not scale up well in large instance spaces (i.e. XGB,SVM,KN,ANN,and LR to a lesser degree) and depending on 'instances' settings, ExSTraCS, eLCS, and XCS)\n","use_uniform_FI = 'True' # (str, True or False) Overides use of any available feature importances estimate methods from models, instead using permutation_importance uniformly\n","primary_metric = 'explained_variance' # (str) Must be an available metric identifier from (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n","\n","# Hyperparameter Sweep Options\n","n_trials = 50   # (int or None) Number of bayesian hyperparameter optimization trials using optuna\n","timeout = 900    # (int or None) Seconds until hyperparameter sweep stops running new trials (Note: it may run longer to finish last trial started)\n","export_hyper_sweep_plots = 'True' # (str, True or False) Export hyper parameter sweep plots from optuna"]},{"cell_type":"markdown","metadata":{"id":"K-ed_hHX8dzj"},"source":["### Hyperparameter Sweep Options for ML Algorithms\n","Users can extend or limit the range or options for given ML algorithm hyperparameters to be tested in hyperparameter optimization. These options are hardcoded when running this pipeline from the command line, but they are available here for users to see and modify. We have sought to include a broad range of relevant configurations based on online examples and relevant research publications. Use caution when modifying values below as improper modifications will lead to pipeline errors/failure. Links to available hyperparameter options for each algorithm are included below."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"772Ljz1h8dzj","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":10,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["def hyperparameters(random_state,feature_names):\n","    param_grid = {}\n","\n","    # DGMM Regressor\n","    # https://github.com/shaoweinuaa/DGMM\n","    param_grid_L21Reg = {'lambda1':[1,100], 'max_iter': [10, 2500]}\n","\n","    param_grid_L21GMMReg = {'lambda1':[1, 300], 'lambda2':[1, 300], 'max_iter': [10, 2500]}\n","\n","    param_grid_L21DGMMReg = {'lambda1': [0.01, 100], 'lambda2': [0.01, 100], 'max_iter': [10, 2500]}\n","\n","    # Elastic Net Regressor\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet\n","    param_grid_EN = {'alpha':[1e-3,1],'max_iter': [10,2500],\n","                     'l1_ratio':[0,1],'random_state':[random_state]}\n","\n","    # Random Forest Regressor\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n","    param_grid_RF = {'n_estimators': [10, 1000],'max_depth': [1, 30],'min_samples_split': [2, 50],\n","                     'min_samples_leaf': [1, 50],'max_features': [None, 'auto', 'log2'],\n","                     'bootstrap': [True],'oob_score': [False, True],'random_state':[random_state]}\n","\n","    # AdaBoost Regressor\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html\n","    param_grid_AdaB = {'n_estimators': [10, 1000], 'learning_rate': [.0001, 0.3], 'loss': ['linear', 'square', 'exponential']}\n","\n","    # GradientBoosting Regressor\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n","    param_grid_GradB = {'learning_rate': [.0001, 0.3],'n_estimators': [10, 1000],\n","                     'min_samples_leaf': [1, 50],'min_samples_split': [2, 50], 'max_depth': [1, 30],\n","                     'random_state':[random_state]}\n","\n","    # Epsilon-Support Vector Regression\n","    # https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n","    param_grid_SVR = {'kernel': ['linear', 'poly', 'rbf'],'C': [0.1, 1000],'gamma': ['scale'],'degree': [1, 6]}\n","\n","    # Group Lasso Regressor\n","    # https://group-lasso.readthedocs.io/en/latest/api_reference.html#\n","    param_grid_GL = {'group_reg':[1e-3,1],#'l1_reg':[0,1],\n","                     'n_iter':[10,2500],\n","                     'scale_reg': ['group_size', 'none', 'inverse_group_size'],\n","                     #'subsampling_scheme': [0.1,0.9],\n","                     #'frobenius_lipschitz': [True],\n","                     'random_state':[random_state]}\n","\n","    #Leave code below as is...\n","    param_grid['L21Reg'] = param_grid_L21Reg\n","    param_grid['L21GMMReg'] = param_grid_L21GMMReg\n","    param_grid['L21DGMMReg'] = param_grid_L21DGMMReg\n","    param_grid['Linear Regression'] = {}\n","    param_grid['Elastic Net'] = param_grid_EN\n","    param_grid['Group Lasso'] = param_grid_GL\n","    param_grid['RF Regressor'] = param_grid_RF\n","    param_grid['AdaBoost'] = param_grid_AdaB\n","    param_grid['GradBoost'] = param_grid_GradB\n","    param_grid['SVR'] = param_grid_SVR\n","    return param_grid"]},{"cell_type":"markdown","metadata":{"id":"92DT87hB8dzl"},"source":["### Run Parameters for Phase 6:  Statistics Summary and Figure Generation"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"DNk8fE7b8dzm","executionInfo":{"status":"ok","timestamp":1688079361694,"user_tz":240,"elapsed":10,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["plot_FI_box = 'True' # (str, True or False) Plot box plot summaries comparing algorithms for each metric\n","plot_metric_boxplots = 'True' # (str, True or False) Plot feature importance boxplots for each algorithm\n","metric_weight = 'explained_variance' # (str, balanced_accuracy or roc_auc) ML model metric used as weight in composite FI plots (only supports balanced_accuracy or roc_auc as options) Recommend setting the same as primary_metric if possible.\n","top_model_features = 40  # (int) Number of top features in model to illustrate in figures"]},{"cell_type":"markdown","metadata":{"id":"q-ftT9-a8dzm"},"source":["### Run Parameters for Phase 10:  Apply Models to Replication Dataset\n","An optional phase to apply all trained models from previous phases to a separate 'replication' dataset which will be used to evaluate models across all algorithms and CV splits. In this demo, we didn't have a separate replication dataset to use for the UCI HCC dataset evaluated. Thus here we use a copy of the original HCC dataset as a 'pretend' replication dataset to demonstrate functionality. The replication data folder can include 1 or more datasets that can be evaluated as separate replication data. The user also needs to"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"05kkozJw8dzm","executionInfo":{"status":"ok","timestamp":1688079361695,"user_tz":240,"elapsed":10,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["applyToReplication = False # (Boolean, True or False) Leave false unless you have a replication dataset handy to further evaluate/compare all models in uniform manner\n","rep_data_path = \"/content/drive/MyDrive/STREAMLINE-main/DemoRepData\" # (txt) Name of folder with replication Dataset(s)\n","dataset_for_rep = \"/content/drive/MyDrive/STREAMLINE-main/DemoRepData/hcc-data_example_rep.csv\" # (txt) Path and name of dataset used to generate the models we want to apply (not the replication dataset)"]},{"cell_type":"markdown","metadata":{"id":"0ne7lHSQ8dzm"},"source":["### Run Parameters for Phase 11:  File Cleanup\n","An optional phase to delete all unnecessary/temporary files generated by the pipeline."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ayVQVBdD8dzm","executionInfo":{"status":"ok","timestamp":1688079361695,"user_tz":240,"elapsed":10,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["del_time = 'True'  # (str, True or False) Delete individual run-time files (but save summary)\n","del_oldCV = 'True' # (str, True or False) Delete any of the older versions of CV training and testing datasets not overwritten (preserves final training and testing datasets)"]},{"cell_type":"markdown","metadata":{"id":"hSLkSNWR8dzn"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 1: Exploratory Analysis"]},{"cell_type":"markdown","metadata":{"id":"SZ8rJH-48dzn"},"source":["### Identify Working Directory"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"kbPBr1Ip8dzn","executionInfo":{"status":"ok","timestamp":1688079361695,"user_tz":240,"elapsed":10,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["wd_path = os.getcwd() #Working directory path automatically detected\n","wd_path = wd_path.replace('\\\\','/')\n","sys.path.insert(1, wd_path+'/streamline')"]},{"cell_type":"markdown","metadata":{"id":"q4APyCi38dzn"},"source":["### Import Python Packages"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"86WT7xUD8dzn","executionInfo":{"status":"ok","timestamp":1688079362182,"user_tz":240,"elapsed":496,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import glob\n","import time\n","import csv\n","import pandas as pd\n","import numpy as np\n","import random\n","import pickle\n","import ExploratoryAnalysisMain\n","import ExploratoryAnalysisJob"]},{"cell_type":"markdown","metadata":{"id":"3gJBsyUr8dzn"},"source":["### Demo Setup\n","Bypasses whatever user may have entered into 'data_path' variable to ensure proper loading of local 'demo' dataset."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ODFpc_e8dzn","executionInfo":{"status":"ok","timestamp":1688079362183,"user_tz":240,"elapsed":19,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}},"outputId":"69140a88-1a06-42af-f1b6-7817778be512"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data Folder Path: /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/Measurements/cdrsb\n"]}],"source":["if demo_run:\n","    data_path = wd_path+'/drive/MyDrive/Master_Thesis/STREAMLINE-main/DemoData'\n","print(\"Data Folder Path: \"+data_path)\n","jupyterRun = 'True' #Leave True or pipeline will not display text or figures"]},{"cell_type":"markdown","metadata":{"id":"nNClA68Sho-L"},"source":["### Imbalance Check (optional)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"utpwi6-ShvsD","executionInfo":{"status":"ok","timestamp":1688079362183,"user_tz":240,"elapsed":17,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["imbalance_check = 'True' #Whether to perform imbalance check or not. Default set to 'False'"]},{"cell_type":"markdown","metadata":{"id":"IFPkZ75q8dzo"},"source":["### Run Exploratory Analysis"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"id":"jP7BKhOZ8dzo","executionInfo":{"status":"error","timestamp":1688079362183,"user_tz":240,"elapsed":17,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}},"outputId":"d4f2b505-bd4b-42a2-c09c-c945e424fcfb"},"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-8bb522afd1c6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mExploratoryAnalysisMain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakeDirTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjupyterRun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/ExploratoryAnalysisMain.py\u001b[0m in \u001b[0;36mmakeDirTree\u001b[0;34m(data_path, output_path, experiment_name, jupyterRun)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Provided data_path does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: A folder with the specified experiment name already exists at \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'. This path/folder name must be unique.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Error: A folder with the specified experiment name already exists at /content/drive/MyDrive/Master_Thesis/STREAMLINE-Regression/Colab_Output/cdrsb_experiment. This path/folder name must be unique."]}],"source":["ExploratoryAnalysisMain.makeDirTree(data_path,output_path,experiment_name,jupyterRun)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_SL1-Pt8dzo","executionInfo":{"status":"aborted","timestamp":1688079362183,"user_tz":240,"elapsed":14,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Determine file extension of datasets in target folder:\n","file_count = 0\n","unique_datanames = []\n","for dataset_path in glob.glob(data_path+'/*'):\n","    dataset_path = str(dataset_path).replace('\\\\','/')\n","    print('---------------------------------------------------------------------------------')\n","    print(dataset_path)\n","    file_extension = dataset_path.split('/')[-1].split('.')[-1]\n","    data_name = dataset_path.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n","    if file_extension == 'txt' or file_extension == 'csv':\n","        if data_name not in unique_datanames:\n","            unique_datanames.append(data_name)\n","            ExploratoryAnalysisJob.runExplore(dataset_path,output_path+'/'+experiment_name,cv_partitions,partition_method,categorical_cutoff,export_feature_correlations,export_univariate_plots,class_label,instance_label,match_label,random_state,ignore_features,categorical_feature_headers,sig_cutoff,jupyterRun)\n","            file_count += 1\n","\n","if file_count == 0: #Check that there was at least 1 dataset\n","    raise Exception(\"There must be at least one .txt or .csv dataset in data_path directory\")\n","\n","#Create metadata dictionary object to keep track of pipeline run paramaters throughout phases\n","metadata = {}\n","metadata['Data Path'] = data_path\n","metadata['Output Path'] = output_path\n","metadata['Experiment Name'] = experiment_name\n","metadata['Class Label'] = class_label\n","metadata['Instance Label'] = instance_label\n","metadata['Ignored Features'] = ignore_features\n","metadata['Specified Categorical Features'] = categorical_feature_headers\n","metadata['CV Partitions'] = cv_partitions\n","metadata['Partition Method'] = partition_method\n","metadata['Match Label'] = match_label\n","metadata['Categorical Cutoff'] = categorical_cutoff\n","metadata['Statistical Significance Cutoff'] = sig_cutoff\n","metadata['Export Feature Correlations'] = export_feature_correlations\n","metadata['Export Univariate Plots'] = export_univariate_plots\n","metadata['Random Seed'] = random_state\n","metadata['Run From Jupyter Notebook'] = jupyterRun\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"TJIt46p38dzq"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 2: Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"VYy-kGwo8dzq"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVZlbWWZ8dzr","executionInfo":{"status":"aborted","timestamp":1688079362184,"user_tz":240,"elapsed":15,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import DataPreprocessingJob"]},{"cell_type":"markdown","metadata":{"id":"b6x5t0B_8dzr"},"source":["### Run Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVuToWNP8dzr","executionInfo":{"status":"aborted","timestamp":1688079362184,"user_tz":240,"elapsed":15,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path+\"/\"+experiment_name)\n","dataset_paths.remove('metadata.pickle')\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path+\"/\"+experiment_name+\"/\"+dataset_directory_path\n","    print(dataset_directory_path)\n","    if dataset_directory_path != 'L21':\n","        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*Train.csv\"):\n","            cv_train_path = str(cv_train_path).replace('\\\\','/')\n","            cv_test_path = cv_train_path.replace(\"Train.csv\",\"Test.csv\")\n","            DataPreprocessingJob.job(cv_train_path,cv_test_path,output_path+'/'+experiment_name,scale_data,impute_data,overwrite_cv,categorical_cutoff,class_label,instance_label,random_state,multi_impute,jupyterRun)\n","\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","\n","#Update metadata\n","metadata['Use Data Scaling'] = scale_data\n","metadata['Use Data Imputation'] = impute_data\n","metadata['Use Multivariate Imputation'] = multi_impute\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"20HLQeHj8dzr"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 3: Feature Importance Evaluation"]},{"cell_type":"markdown","metadata":{"id":"GVPgVV2v8dzr"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlpP3ubp8dzr","executionInfo":{"status":"aborted","timestamp":1688079362184,"user_tz":240,"elapsed":15,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import FeatureImportanceJob"]},{"cell_type":"markdown","metadata":{"id":"Y6ZQpr2_8dzr"},"source":["### Run Feature Importance Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7_le_0I8dzs","executionInfo":{"status":"aborted","timestamp":1688079362184,"user_tz":240,"elapsed":14,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path+\"/\"+experiment_name)\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path+\"/\"+experiment_name+\"/\"+dataset_directory_path\n","    experiment_path = output_path+'/'+experiment_name\n","\n","    if eval(do_mutual_info) or eval(do_multisurf):\n","        if not os.path.exists(full_path+\"/feature_selection\"):\n","            os.mkdir(full_path+\"/feature_selection\")\n","\n","    if eval(do_mutual_info):\n","        if not os.path.exists(full_path+\"/feature_selection/mutualinformation\"):\n","            os.mkdir(full_path+\"/feature_selection/mutualinformation\")\n","        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*_CV_*Train.csv\"):\n","            cv_train_path = str(cv_train_path).replace('\\\\','/')\n","            FeatureImportanceJob.job(cv_train_path,experiment_path,random_state,class_label,instance_label,instance_subset,'mi',njobs,use_TURF,TURF_pct,jupyterRun)\n","\n","    if eval(do_multisurf):\n","        if not os.path.exists(full_path+\"/feature_selection/multisurf\"):\n","            os.mkdir(full_path+\"/feature_selection/multisurf\")\n","        for cv_train_path in glob.glob(full_path+\"/CVDatasets/*_CV_*Train.csv\"):\n","            cv_train_path = str(cv_train_path).replace('\\\\','/')\n","            FeatureImportanceJob.job(cv_train_path,experiment_path,random_state,class_label,instance_label,instance_subset,'ms',njobs,use_TURF,TURF_pct,jupyterRun)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","\n","#Update metadata\n","metadata['Use Mutual Information'] = do_mutual_info\n","metadata['Use MultiSURF'] = do_multisurf\n","metadata['Use TURF'] = use_TURF\n","metadata['TURF Cutoff'] = TURF_pct\n","metadata['MultiSURF Instance Subset'] = instance_subset\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkNW0Wz7diwW","executionInfo":{"status":"aborted","timestamp":1688079362184,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if do_L21Series == 'True':\n","  path_1 = output_path + \"/\" + experiment_name\n","  os.mkdir(path_1+'/L21')\n","  for dataset_directory_path_1 in dataset_paths:\n","      dest_dir = path_1+'/L21'+'/'+dataset_directory_path_1\n","      src_dir = path_1+'/'+dataset_directory_path_1\n","      shutil.copytree(src_dir, dest_dir)"]},{"cell_type":"markdown","metadata":{"id":"AxziLxaL8dzs"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 4: Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"KZFG118c8dzs"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uq8LuYM28dzs","executionInfo":{"status":"aborted","timestamp":1688079362184,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import FeatureSelectionJob"]},{"cell_type":"markdown","metadata":{"id":"EHk7d8sa8dzs"},"source":["### Run Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiUE_8KK8dzt","executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n","removeList = removeList = ['L21', 'metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","\n","for dataset_directory_path in dataset_paths:\n","    full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n","    FeatureSelectionJob.job(full_path,do_mutual_info,do_multisurf,max_features_to_keep,filter_poor_features,top_features,export_scores,class_label,instance_label,cv_partitions,overwrite_cv,jupyterRun)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","\n","#Update metadata\n","metadata['Max Features to Keep'] = max_features_to_keep\n","metadata['Filter Poor Features'] = filter_poor_features\n","metadata['Top Features to Display'] = top_features\n","metadata['Export Feature Importance Plot'] = export_scores\n","metadata['Overwrite CV Datasets'] = overwrite_cv\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"6Z2mquRz8dzt"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 5: ML Modeling"]},{"cell_type":"markdown","metadata":{"id":"HP2f9wxN8dzt"},"source":["### Phase 5 Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evsXRs-A8dzt","executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import ModelJob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTMUZPCP8dzt","executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Create ML modeling algorithm information dictionary, given as ['algorithm used (set to true initially by default)','algorithm abreviation', 'color used for algorithm on figures']\n","### Note that other named colors used by matplotlib can be found here: https://matplotlib.org/3.5.0/_images/sphx_glr_named_colors_003.png\n","### Make sure new ML algorithm abbreviations and color designations are unique\n","algInfo = {}\n","\n","algInfo['Linear Regression'] = [True,'Linear Regression','red']\n","algInfo['Elastic Net'] = [True, 'Elastic Net', 'steelblue']\n","algInfo['Group Lasso'] = [True, 'Group Lasso', 'orange']\n","algInfo['L21Reg'] = [True,'L21Reg','green']\n","algInfo['L21GMMReg'] = [True, 'L21GMMReg', 'darkslategray']\n","algInfo['L21DGMMReg'] = [True, 'L21DGMMReg', 'magenta']\n","\n","algInfo['RF Regressor'] = [True, 'RF Regressor', 'navy']\n","algInfo['AdaBoost'] = [True, 'AdaBoost', 'teal']\n","algInfo['GradBoost'] = [True, 'GradBoost', 'olive']\n","algInfo['SVR'] = [True, 'SVR', 'rosybrown']\n","### Add new algorithms here...\n","\n","\n","#Set up ML algorithm True/False use\n","if not eval(do_all): #If do all algorithms is false\n","    for key in algInfo:\n","        algInfo[key][0] = False #Set algorithm use to False\n","\n","#Set algorithm use truth for each algorithm specified by user (i.e. if user specified True/False for a specific algorithm)\n","if not do_linReg == 'None':\n","    algInfo['Linear Regression'][0] = eval(do_linReg)\n","if not do_ENReg == 'None':\n","    algInfo['Elastic Net'][0] = eval(do_ENReg)\n","if not do_GL == 'None':\n","    algInfo['Group Lasso'][0] = eval(do_GL)\n","if not do_L21Reg == 'None':\n","    algInfo['L21Reg'][0] = eval(do_L21Reg)\n","if not do_L21GMMReg == 'None':\n","    algInfo['L21GMMReg'][0] = eval(do_L21GMMReg)\n","if not do_L21DGMMReg == 'None':\n","    algInfo['L21DGMMReg'][0] = eval(do_L21DGMMReg)\n","\n","if not do_RFReg == 'None':\n","    algInfo['RF Regressor'][0] = eval(do_RFReg)\n","if not do_AdaReg == 'None':\n","    algInfo['AdaBoost'][0] = eval(do_AdaReg)\n","if not do_GradReg == 'None':\n","    algInfo['GradBoost'][0] = eval(do_GradReg)\n","if not do_SVR == 'None':\n","    algInfo['SVR'][0] = eval(do_SVR)\n","### Add new algorithms here...\n","\n","\n","\n","#Pickle the algorithm information dictionary for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"algInfo.pickle\", 'wb')\n","pickle.dump(algInfo,pickle_out)\n","pickle_out.close()\n","\n","#Make list of algorithms to be run (full names)\n","algorithms = []\n","for key in algInfo:\n","    if algInfo[key][0]: #Algorithm is true\n","        algorithms.append(key)"]},{"cell_type":"markdown","metadata":{"id":"TKAL_ARc8dzt"},"source":["### Run ML Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkmC-G-X8dzu","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n","removeList = removeList = ['L21', 'metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","print(dataset_paths)\n","if do_CommonReg == 'True':\n","  for dataset_directory_path in dataset_paths:\n","      full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n","      if not os.path.exists(full_path+'/models'):\n","          os.mkdir(full_path+'/models')\n","      if not os.path.exists(full_path+'/model_evaluation'):\n","          os.mkdir(full_path+'/model_evaluation')\n","      if not os.path.exists(full_path+'/models/pickledModels'):\n","          os.mkdir(full_path+'/models/pickledModels')\n","\n","      for cvCount in range(cv_partitions):\n","          train_file_path = full_path+'/CVDatasets/'+dataset_directory_path+\"_CV_\"+str(cvCount)+\"_Train.csv\"\n","          test_file_path = full_path + '/CVDatasets/' + dataset_directory_path + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n","          for algorithm in algorithms:\n","              if algorithm != 'L21Reg' and algorithm !='L21GMMReg' and algorithm !='L21DGMMReg':\n","                  print(algorithm)\n","                  algAbrev = algInfo[algorithm][1]\n","                  #Get header names for current CV dataset for use later in GP tree visulaization\n","                  data_name = full_path.split('/')[-1]\n","                  feature_names = pd.read_csv(full_path+'/CVDatasets/'+data_name+'_CV_'+str(cvCount)+'_Test.csv').columns.values.tolist()\n","                  if instance_label != 'None':\n","                      feature_names.remove(instance_label)\n","                  feature_names.remove(class_label)\n","                  #Get hyperparameter grid\n","                  param_grid = hyperparameters(random_state,feature_names)[algorithm]\n","                  ModelJob.runModel(algorithm,train_file_path,test_file_path,full_path,n_trials,timeout,export_hyper_sweep_plots,instance_label,class_label,random_state,cvCount,filter_poor_features,training_subsample,use_uniform_FI,primary_metric,param_grid,groups_path, algAbrev)\n","\n","if do_L21Series == 'True':\n","  full_path = output_path + \"/\" + experiment_name + \"/\" + 'L21'\n","  for cvCount in range(cv_partitions):\n","      dataset_paths_2 = dataset_paths.copy()\n","      train_file_path_1 = full_path+'/'+dataset_paths[0]+'/CVDatasets/'+dataset_paths_2[0]+\"_CV_\"+str(cvCount)+\"_Train.csv\"\n","      test_file_path_1 = full_path+'/'+dataset_paths[0]+ '/CVDatasets/' + dataset_paths_2[0] + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n","      train_file_path_2 = full_path+'/'+dataset_paths[1]+'/CVDatasets/'+dataset_paths_2[1]+\"_CV_\"+str(cvCount)+\"_Train.csv\"\n","      test_file_path_2 = full_path +'/'+dataset_paths[1]+ '/CVDatasets/' + dataset_paths_2[1] + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n","      train_file_path_3 = full_path+'/'+dataset_paths[2]+'/CVDatasets/'+dataset_paths_2[2]+\"_CV_\"+str(cvCount)+\"_Train.csv\"\n","      test_file_path_3 = full_path +'/'+dataset_paths[2]+ '/CVDatasets/' + dataset_paths_2[2] + \"_CV_\" + str(cvCount) + \"_Test.csv\"\n","      for algorithm in algorithms:\n","        if algorithm == 'L21Reg' or algorithm =='L21GMMReg' or algorithm =='L21DGMMReg':\n","            print(algorithm)\n","            algAbrev = algInfo[algorithm][1]\n","            #Get header names for current CV dataset for use later in GP tree visulaization\n","            data_name = full_path.split('/')[-1]\n","            feature_names = pd.read_csv(full_path+'/'+dataset_paths[0]+'/CVDatasets/'+dataset_paths_2[0]+'_CV_'+str(cvCount)+'_Test.csv').columns.values.tolist()\n","            if instance_label != 'None':\n","                feature_names.remove(instance_label)\n","            feature_names.remove(class_label)\n","            #Get hyperparameter grid\n","            param_grid = hyperparameters(random_state,feature_names)[algorithm]\n","            ModelJob.runModel_2(algorithm,train_file_path_1, train_file_path_2, train_file_path_3,test_file_path_1, test_file_path_2, test_file_path_3,full_path,n_trials,timeout,export_hyper_sweep_plots,instance_label,class_label,random_state,cvCount,filter_poor_features,training_subsample,use_uniform_FI,primary_metric,param_grid,algAbrev, output_path, experiment_name)\n","\n","#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","\n","#Update metadata\n","### Add new algorithms here...\n","metadata['Linear Regression'] = str(algInfo['Linear Regression'][0])\n","metadata['Elastic Net'] = str(algInfo['Elastic Net'][0])\n","metadata['Group Lasso'] = str(algInfo['Group Lasso'][0])\n","metadata['L21Reg'] = str(algInfo['L21Reg'][0])\n","metadata['L21GMMReg'] = str(algInfo['L21GMMReg'][0])\n","metadata['L21DGMMReg'] = str(algInfo['L21DGMMReg'][0])\n","\n","\n","metadata['Primary Metric'] = primary_metric\n","metadata['Training Subsample for KNN,ANN,SVM,and XGB'] = training_subsample\n","metadata['Uniform Feature Importance Estimation (Models)'] = use_uniform_FI\n","metadata['Hyperparameter Sweep Number of Trials'] = n_trials\n","metadata['Hyperparameter Sweep Number of Trials'] = n_trials\n","metadata['Hyperparameter Timeout'] = timeout\n","metadata['Export Hyperparameter Sweep Plots'] = export_hyper_sweep_plots\n","\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()"]},{"cell_type":"markdown","metadata":{"id":"wH3spCnU8dzu"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 6: Statistics (Stats Summaries, Figures, Statistical Comparisons)"]},{"cell_type":"markdown","metadata":{"id":"rORNdZ3n8dzu"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52JPBP0W8dzu","executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import StatsJob"]},{"cell_type":"markdown","metadata":{"id":"POVmBAuE8dzu"},"source":["### Run Statistics Summary and Figure Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13jEzsWH8dzu","scrolled":true,"executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["#Unpickle metadata from previous phase\n","file = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'rb')\n","metadata = pickle.load(file)\n","file.close()\n","metadata['Export Metric Boxplots'] = plot_metric_boxplots\n","metadata['Export Feature Importance Boxplots'] = plot_FI_box\n","metadata['Metric Weighting Composite FI Plots'] = metric_weight\n","metadata['Top Model Features To Display'] = top_model_features\n","#Pickle the metadata for future use\n","pickle_out = open(output_path+'/'+experiment_name+'/'+\"metadata.pickle\", 'wb')\n","pickle.dump(metadata,pickle_out)\n","pickle_out.close()\n","\n","#Now that primary pipeline phases are complete generate a human readable version of metadata\n","df = pd.DataFrame.from_dict(metadata, orient ='index')\n","df.to_csv(output_path+'/'+experiment_name+'/'+'metadata.csv',index=True)\n","\n","# Iterate through datasets\n","dataset_paths = os.listdir(output_path + \"/\" + experiment_name)\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in dataset_paths:\n","        dataset_paths.remove(text)\n","for dataset_directory_path in dataset_paths:\n","    if dataset_directory_path != 'L21':\n","        full_path = output_path + \"/\" + experiment_name + \"/\" + dataset_directory_path\n","        StatsJob.job(full_path,plot_FI_box,class_label,instance_label,cv_partitions,scale_data,plot_metric_boxplots,primary_metric,top_model_features,sig_cutoff,metric_weight,jupyterRun)\n"]},{"cell_type":"markdown","metadata":{"id":"7maM_tZ68dzu"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 7: Dataset Comparison (Optional: Use only if > 1 dataset was analyzed)"]},{"cell_type":"markdown","metadata":{"id":"vxvVpqQT8dzv"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENiuJTHr8dzv","executionInfo":{"status":"aborted","timestamp":1688079362185,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import DataCompareJob"]},{"cell_type":"markdown","metadata":{"id":"1LzGjuz_8dzv"},"source":["### Run Dataset Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F74vgxAi8dzv","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if do_L21Series == 'True':\n","    dataset_paths.remove('L21')\n","if len(dataset_paths) > 1:\n","    DataCompareJob.job(output_path+'/'+experiment_name,sig_cutoff,jupyterRun)"]},{"cell_type":"markdown","metadata":{"id":"9ym5UNON8dzv"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 8: PDF Training Report Generator (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"apShjsZ3ZO1b","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import PDF_ReportJob_Reg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIZwGqGtZR6O","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["experiment_path = output_path+'/'+experiment_name\n","PDF_ReportJob_Reg.job(experiment_path,'True','None','None')"]},{"cell_type":"markdown","metadata":{"id":"L1itH5nZ8dzv"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 9: Apply Models to Replication Data (Optional)"]},{"cell_type":"markdown","metadata":{"id":"WmCSgeNS8dzw"},"source":["### Import Additional Python Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjfHE3rT8dzw","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import ApplyModelJob"]},{"cell_type":"markdown","metadata":{"id":"a4_lLLea8dzw"},"source":["### Specify Run Parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Fc3nUse8dzw","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if demo_run:\n","    rep_data_path = wd_path+'/drive/MyDrive/STREAMLINE-main/DemoRepData'\n","print(\"Replication Data Folder Path: \"+rep_data_path)\n","print(\"Dataset Path: \"+dataset_for_rep)"]},{"cell_type":"markdown","metadata":{"id":"yn4BqG7E8dzw"},"source":["### Run Application of Models to Replication Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oa9-vewM8dzw","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if applyToReplication:\n","    data_name = dataset_for_rep.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n","    full_path = output_path + \"/\" + experiment_name + \"/\" + data_name #location of folder containing models respective training dataset\n","    full_path\n","    # full_path_2 = output_path + \"/\" + experiment_name + \"/\" + data_name\n","    if not os.path.exists(full_path):\n","        os.mkdir(full_path)\n","    if not os.path.exists(full_path+\"/applymodel\"):\n","        os.mkdir(full_path+\"/applymodel\")\n","\n","    #Determine file extension of datasets in target folder:\n","    file_count = 0\n","    unique_datanames = []\n","    for datasetFilename in glob.glob(rep_data_path+'/*'):\n","        datasetFilename = str(datasetFilename).replace('\\\\','/')\n","\n","        file_extension = datasetFilename.split('/')[-1].split('.')[-1]\n","        apply_name = datasetFilename.split('/')[-1].split('.')[0] #Save unique dataset names so that analysis is run only once if there is both a .txt and .csv version of dataset with same name.\n","        if not os.path.exists(full_path+\"/applymodel/\"+apply_name):\n","            os.mkdir(full_path+\"/applymodel/\"+apply_name)\n","\n","        if file_extension == 'txt' or file_extension == 'csv':\n","            if apply_name not in unique_datanames:\n","                unique_datanames.append(apply_name)\n","                ApplyModelJob.job(datasetFilename,full_path,class_label,instance_label,categorical_cutoff,sig_cutoff,cv_partitions,scale_data,impute_data,primary_metric,dataset_for_rep,match_label,plot_ROC,plot_PRC,plot_metric_boxplots,export_feature_correlations,jupyterRun,multi_impute)\n","                file_count += 1\n","\n","    if file_count == 0: #Check that there was at least 1 dataset\n","        raise Exception(\"There must be at least one .txt or .csv dataset in rep_data_path directory\")"]},{"cell_type":"markdown","metadata":{"id":"_JLQNC5h8dzw"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 10: PDF Apply Report Generator (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhQNBsBX8dzx","executionInfo":{"status":"aborted","timestamp":1688079362186,"user_tz":240,"elapsed":12,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["import PDF_ReportJob"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1E5e4AIt8dzx","executionInfo":{"status":"aborted","timestamp":1688079362187,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["if applyToReplication:\n","    experiment_path = output_path+'/'+experiment_name\n","    PDF_ReportJob.job(experiment_path,'False',rep_data_path,dataset_for_rep)"]},{"cell_type":"markdown","metadata":{"id":"RT80Ky968dzx"},"source":["## -----------------------------------------------------------------------------------------------------------------\n","## Phase 11: File Cleanup (Optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnpxRcR28dzx","executionInfo":{"status":"aborted","timestamp":1688079362187,"user_tz":240,"elapsed":13,"user":{"displayName":"Yanbo Feng","userId":"17225910291528625753"}}},"outputs":[],"source":["# Get dataset paths for all completed dataset analyses in experiment folder\n","datasets = os.listdir(experiment_path)\n","experiment_name = experiment_path.split('/')[-1] #Name of experiment folder\n","removeList = removeList = ['metadata.pickle','metadata.csv','algInfo.pickle','jobsCompleted','logs','jobs','DatasetComparisons','UsefulNotebooks',experiment_name+'_ML_Pipeline_Report.pdf']\n","for text in removeList:\n","    if text in datasets:\n","        datasets.remove(text)\n","\n","#Delete jobscompleted folder/files\n","try:\n","    shutil.rmtree(experiment_path+'/'+'jobsCompleted')\n","except:\n","    pass\n","\n","#Delete target files within each dataset subfolder\n","for dataset in datasets:\n","    #Delete individual runtime files (save runtime summary generated in phase 6)\n","    if eval(del_time):\n","        try:\n","            shutil.rmtree(experiment_path+'/'+dataset+'/'+'runtime')\n","            print(\"Individual Runtime Files Deleted\")\n","        except:\n","            pass\n","    #Delete temporary feature importance pickle files (only needed for phase 4 and then saved as summary files in phase 6)\n","    try:\n","        shutil.rmtree(experiment_path+'/'+dataset+'/feature_selection/mutualinformation/pickledForPhase4')\n","        print(\"Mutual Information Pickle Files Deleted\")\n","    except:\n","        pass\n","    try:\n","        shutil.rmtree(experiment_path+'/'+dataset+'/feature_selection/multisurf/pickledForPhase4')\n","        print(\"MultiSURF Pickle Files Deleted\")\n","    except:\n","        pass\n","    #Delete older training and testing CV datasets (does not delete any final versions used for training). Older cv datasets might have been kept to see what they look like prior to preprocessing and feature selection.\n","    if eval(del_oldCV):\n","        #Delete CV files generated after preprocessing but before feature selection\n","        files = glob.glob(experiment_path+'/'+dataset+'/CVDatasets/*CVOnly*')\n","        for f in files:\n","            try:\n","                os.remove(f)\n","                print(\"Deleted Intermediary CV-Only Dataset Files\")\n","            except:\n","                pass\n","        #Delete CV files generated after CV partitioning but before preprocessing\n","        files = glob.glob(experiment_path+'/'+dataset+'/CVDatasets/*CVPre*')\n","        for f in files:\n","            try:\n","                os.remove(f)\n","                print(\"Deleted Intermediary CV-Pre Dataset Files\")\n","            except:\n","                pass"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}