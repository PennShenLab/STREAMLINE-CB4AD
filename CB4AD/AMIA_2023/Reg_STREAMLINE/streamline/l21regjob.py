# -*- coding: utf-8 -*-
"""L21RegJob.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sXbrfkQ_h-UlIRg6esy-ZZ6i1r6xylM8
"""
import networkx as nx
import copy
import warnings
from collections import defaultdict
import platform
import inspect
import re
import xlrd
import math
import collections
import numpy as np
import pandas as pd
from numpy import linalg as LA
import matplotlib.pyplot as plt
from collections import Counter
from scipy.stats import pearsonr
from sklearn import linear_model
import seaborn as sns; sns.set_theme()
from sklearn.model_selection import ShuffleSplit

class BaseEstimator:
    """Base class for all estimators in scikit-learn.
    Notes
    -----
    All estimators should specify all the parameters that can be set
    at the class level in their ``__init__`` as explicit keyword
    arguments (no ``*args`` or ``**kwargs``).
    """

    @classmethod
    def _get_param_names(cls):
        """Get parameter names for the estimator"""
        # fetch the constructor or the original constructor before
        # deprecation wrapping if any
        init = getattr(cls.__init__, "deprecated_original", cls.__init__)
        if init is object.__init__:
            # No explicit constructor to introspect
            return []

        # introspect the constructor arguments to find the model parameters
        # to represent
        init_signature = inspect.signature(init)
        # Consider the constructor parameters excluding 'self'
        parameters = [
            p
            for p in init_signature.parameters.values()
            if p.name != "self" and p.kind != p.VAR_KEYWORD
        ]
        for p in parameters:
            if p.kind == p.VAR_POSITIONAL:
                raise RuntimeError(
                    "scikit-learn estimators should always "
                    "specify their parameters in the signature"
                    " of their __init__ (no varargs)."
                    " %s with constructor %s doesn't "
                    " follow this convention." % (cls, init_signature)
                )
        # Extract and sort argument names excluding 'self'
        return sorted([p.name for p in parameters])

    def get_params(self, deep=True):
        """
        Get parameters for this estimator.
        Parameters
        ----------
        deep : bool, default=True
            If True, will return the parameters for this estimator and
            contained subobjects that are estimators.
        Returns
        -------
        params : dict
            Parameter names mapped to their values.
        """
        out = dict()
        for key in self._get_param_names():
            value = getattr(self, key)
            if deep and hasattr(value, "get_params"):
                deep_items = value.get_params().items()
                out.update((key + "__" + k, val) for k, val in deep_items)
            out[key] = value
        return out

    def set_params(self, **params):
        """Set the parameters of this estimator.
        The method works on simple estimators as well as on nested objects
        (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
        parameters of the form ``<component>__<parameter>`` so that it's
        possible to update each component of a nested object.
        Parameters
        ----------
        **params : dict
            Estimator parameters.
        Returns
        -------
        self : estimator instance
            Estimator instance.
        """
        if not params:
            # Simple optimization to gain speed (inspect is slow)
            return self
        valid_params = self.get_params(deep=True)

        nested_params = defaultdict(dict)  # grouped by prefix
        for key, value in params.items():
            key, delim, sub_key = key.partition("__")
            if key not in valid_params:
                local_valid_params = self._get_param_names()
                raise ValueError(
                    f"Invalid parameter {key!r} for estimator {self}. "
                    f"Valid parameters are: {local_valid_params!r}."
                )

            if delim:
                nested_params[key][sub_key] = value
            else:
                setattr(self, key, value)
                valid_params[key] = value

        for key, sub_params in nested_params.items():
            valid_params[key].set_params(**sub_params)

        return self

    def __repr__(self, N_CHAR_MAX=700):
        # N_CHAR_MAX is the (approximate) maximum number of non-blank
        # characters to render. We pass it as an optional parameter to ease
        # the tests.

        from .utils._pprint import _EstimatorPrettyPrinter

        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences

        # use ellipsis for sequences with a lot of elements
        pp = _EstimatorPrettyPrinter(
            compact=True,
            indent=1,
            indent_at_name=True,
            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW,
        )

        repr_ = pp.pformat(self)

        # Use bruteforce ellipsis when there are a lot of non-blank characters
        n_nonblank = len("".join(repr_.split()))
        if n_nonblank > N_CHAR_MAX:
            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends
            regex = r"^(\s*\S){%d}" % lim
            # The regex '^(\s*\S){%d}' % n
            # matches from the start of the string until the nth non-blank
            # character:
            # - ^ matches the start of string
            # - (pattern){n} matches n repetitions of pattern
            # - \s*\S matches a non-blank char following zero or more blanks
            left_lim = re.match(regex, repr_).end()
            right_lim = re.match(regex, repr_[::-1]).end()

            if "\n" in repr_[left_lim:-right_lim]:
                # The left side and right side aren't on the same line.
                # To avoid weird cuts, e.g.:
                # categoric...ore',
                # we need to start the right side with an appropriate newline
                # character so that it renders properly as:
                # categoric...
                # handle_unknown='ignore',
                # so we add [^\n]*\n which matches until the next \n
                regex += r"[^\n]*\n"
                right_lim = re.match(regex, repr_[::-1]).end()

            ellipsis = "..."
            if left_lim + len(ellipsis) < len(repr_) - right_lim:
                # Only add ellipsis if it results in a shorter repr
                repr_ = repr_[:left_lim] + "..." + repr_[-right_lim:]

        return repr_

    def __getstate__(self):
        try:
            state = super().__getstate__()
        except AttributeError:
            state = self.__dict__.copy()

        if type(self).__module__.startswith("sklearn."):
            return dict(state.items(), _sklearn_version=__version__)
        else:
            return state

    def __setstate__(self, state):
        if type(self).__module__.startswith("sklearn."):
            pickle_version = state.pop("_sklearn_version", "pre-0.18")
            if pickle_version != __version__:
                warnings.warn(
                    "Trying to unpickle estimator {0} from version {1} when "
                    "using version {2}. This might lead to breaking code or "
                    "invalid results. Use at your own risk. "
                    "For more info please refer to:\n"
                    "https://scikit-learn.org/stable/model_persistence.html"
                    "#security-maintainability-limitations".format(
                        self.__class__.__name__, pickle_version, __version__
                    ),
                    UserWarning,
                )
        try:
            super().__setstate__(state)
        except AttributeError:
            self.__dict__.update(state)

    def _more_tags(self):
        return _DEFAULT_TAGS

    def _get_tags(self):
        collected_tags = {}
        for base_class in reversed(inspect.getmro(self.__class__)):
            if hasattr(base_class, "_more_tags"):
                # need the if because mixins might not have _more_tags
                # but might do redundant work in estimators
                # (i.e. calling more tags on BaseEstimator multiple times)
                more_tags = base_class._more_tags(self)
                collected_tags.update(more_tags)
        return collected_tags

    def _check_n_features(self, X, reset):
        """Set the `n_features_in_` attribute, or check against it.
        Parameters
        ----------
        X : {ndarray, sparse matrix} of shape (n_samples, n_features)
            The input samples.
        reset : bool
            If True, the `n_features_in_` attribute is set to `X.shape[1]`.
            If False and the attribute exists, then check that it is equal to
            `X.shape[1]`. If False and the attribute does *not* exist, then
            the check is skipped.
            .. note::
               It is recommended to call reset=True in `fit` and in the first
               call to `partial_fit`. All other methods that validate `X`
               should set `reset=False`.
        """
        try:
            n_features = _num_features(X)
        except TypeError as e:
            if not reset and hasattr(self, "n_features_in_"):
                raise ValueError(
                    "X does not contain any features, but "
                    f"{self.__class__.__name__} is expecting "
                    f"{self.n_features_in_} features"
                ) from e
            # If the number of features is not defined and reset=True,
            # then we skip this check
            return

        if reset:
            self.n_features_in_ = n_features
            return

        if not hasattr(self, "n_features_in_"):
            # Skip this check if the expected number of expected input features
            # was not recorded by calling fit first. This is typically the case
            # for stateless transformers.
            return

        if n_features != self.n_features_in_:
            raise ValueError(
                f"X has {n_features} features, but {self.__class__.__name__} "
                f"is expecting {self.n_features_in_} features as input."
            )

    def _check_feature_names(self, X, *, reset):
        """Set or check the `feature_names_in_` attribute.
        .. versionadded:: 1.0
        Parameters
        ----------
        X : {ndarray, dataframe} of shape (n_samples, n_features)
            The input samples.
        reset : bool
            Whether to reset the `feature_names_in_` attribute.
            If False, the input will be checked for consistency with
            feature names of data provided when reset was last True.
            .. note::
               It is recommended to call `reset=True` in `fit` and in the first
               call to `partial_fit`. All other methods that validate `X`
               should set `reset=False`.
        """

        if reset:
            feature_names_in = _get_feature_names(X)
            if feature_names_in is not None:
                self.feature_names_in_ = feature_names_in
            elif hasattr(self, "feature_names_in_"):
                # Delete the attribute when the estimator is fitted on a new dataset
                # that has no feature names.
                delattr(self, "feature_names_in_")
            return

        fitted_feature_names = getattr(self, "feature_names_in_", None)
        X_feature_names = _get_feature_names(X)

        if fitted_feature_names is None and X_feature_names is None:
            # no feature names seen in fit and in X
            return

        if X_feature_names is not None and fitted_feature_names is None:
            warnings.warn(
                f"X has feature names, but {self.__class__.__name__} was fitted without"
                " feature names"
            )
            return

        if X_feature_names is None and fitted_feature_names is not None:
            warnings.warn(
                "X does not have valid feature names, but"
                f" {self.__class__.__name__} was fitted with feature names"
            )
            return

        # validate the feature names against the `feature_names_in_` attribute
        if len(fitted_feature_names) != len(X_feature_names) or np.any(
            fitted_feature_names != X_feature_names
        ):
            message = (
                "The feature names should match those that were "
                "passed during fit. Starting version 1.2, an error will be raised.\n"
            )
            fitted_feature_names_set = set(fitted_feature_names)
            X_feature_names_set = set(X_feature_names)

            unexpected_names = sorted(X_feature_names_set - fitted_feature_names_set)
            missing_names = sorted(fitted_feature_names_set - X_feature_names_set)

            def add_names(names):
                output = ""
                max_n_names = 5
                for i, name in enumerate(names):
                    if i >= max_n_names:
                        output += "- ...\n"
                        break
                    output += f"- {name}\n"
                return output

            if unexpected_names:
                message += "Feature names unseen at fit time:\n"
                message += add_names(unexpected_names)

            if missing_names:
                message += "Feature names seen at fit time, yet now missing:\n"
                message += add_names(missing_names)

            if not missing_names and not unexpected_names:
                message += (
                    "Feature names must be in the same order as they were in fit.\n"
                )

            warnings.warn(message, FutureWarning)

    def _validate_data(
        self,
        X="no_validation",
        y="no_validation",
        reset=True,
        validate_separately=False,
        **check_params,
    ):
        """Validate input data and set or check the `n_features_in_` attribute.
        Parameters
        ----------
        X : {array-like, sparse matrix, dataframe} of shape \
                (n_samples, n_features), default='no validation'
            The input samples.
            If `'no_validation'`, no validation is performed on `X`. This is
            useful for meta-estimator which can delegate input validation to
            their underlying estimator(s). In that case `y` must be passed and
            the only accepted `check_params` are `multi_output` and
            `y_numeric`.
        y : array-like of shape (n_samples,), default='no_validation'
            The targets.
            - If `None`, `check_array` is called on `X`. If the estimator's
              requires_y tag is True, then an error will be raised.
            - If `'no_validation'`, `check_array` is called on `X` and the
              estimator's requires_y tag is ignored. This is a default
              placeholder and is never meant to be explicitly set. In that case
              `X` must be passed.
            - Otherwise, only `y` with `_check_y` or both `X` and `y` are
              checked with either `check_array` or `check_X_y` depending on
              `validate_separately`.
        reset : bool, default=True
            Whether to reset the `n_features_in_` attribute.
            If False, the input will be checked for consistency with data
            provided when reset was last True.
            .. note::
               It is recommended to call reset=True in `fit` and in the first
               call to `partial_fit`. All other methods that validate `X`
               should set `reset=False`.
        validate_separately : False or tuple of dicts, default=False
            Only used if y is not None.
            If False, call validate_X_y(). Else, it must be a tuple of kwargs
            to be used for calling check_array() on X and y respectively.
            `estimator=self` is automatically added to these dicts to generate
            more informative error message in case of invalid input data.
        **check_params : kwargs
            Parameters passed to :func:`sklearn.utils.check_array` or
            :func:`sklearn.utils.check_X_y`. Ignored if validate_separately
            is not False.
            `estimator=self` is automatically added to these params to generate
            more informative error message in case of invalid input data.
        Returns
        -------
        out : {ndarray, sparse matrix} or tuple of these
            The validated input. A tuple is returned if both `X` and `y` are
            validated.
        """
        self._check_feature_names(X, reset=reset)

        if y is None and self._get_tags()["requires_y"]:
            raise ValueError(
                f"This {self.__class__.__name__} estimator "
                "requires y to be passed, but the target y is None."
            )

        no_val_X = isinstance(X, str) and X == "no_validation"
        no_val_y = y is None or isinstance(y, str) and y == "no_validation"

        default_check_params = {"estimator": self}
        check_params = {**default_check_params, **check_params}

        if no_val_X and no_val_y:
            raise ValueError("Validation should be done on X, y or both.")
        elif not no_val_X and no_val_y:
            X = check_array(X, input_name="X", **check_params)
            out = X
        elif no_val_X and not no_val_y:
            y = _check_y(y, **check_params)
            out = y
        else:
            if validate_separately:
                # We need this because some estimators validate X and y
                # separately, and in general, separately calling check_array()
                # on X and y isn't equivalent to just calling check_X_y()
                # :(
                check_X_params, check_y_params = validate_separately
                if "estimator" not in check_X_params:
                    check_X_params = {**default_check_params, **check_X_params}
                X = check_array(X, input_name="X", **check_X_params)
                if "estimator" not in check_y_params:
                    check_y_params = {**default_check_params, **check_y_params}
                y = check_array(y, input_name="y", **check_y_params)
            else:
                X, y = check_X_y(X, y, **check_params)
            out = X, y

        if not no_val_X and check_params.get("ensure_2d", True):
            self._check_n_features(X, reset=reset)

        return out

    @property
    def _repr_html_(self):
        """HTML representation of estimator.
        This is redundant with the logic of `_repr_mimebundle_`. The latter
        should be favorted in the long term, `_repr_html_` is only
        implemented for consumers who do not interpret `_repr_mimbundle_`.
        """
        if get_config()["display"] != "diagram":
            raise AttributeError(
                "_repr_html_ is only defined when the "
                "'display' configuration option is set to "
                "'diagram'"
            )
        return self._repr_html_inner

    def _repr_html_inner(self):
        """This function is returned by the @property `_repr_html_` to make
        `hasattr(estimator, "_repr_html_") return `True` or `False` depending
        on `get_config()["display"]`.
        """
        return estimator_html_repr(self)

    def _repr_mimebundle_(self, **kwargs):
        """Mime bundle used by jupyter kernels to display estimator"""
        output = {"text/plain": repr(self)}
        if get_config()["display"] == "diagram":
            output["text/html"] = estimator_html_repr(self)
        return output

def f_lapLabelDistMatrix(X,Y):
  K = X.shape[0]
  ld = len(Y)
  tempD = np.diag(np.ones(ld,))
  CC = []
  L = []
  
  for k in range(0,K):
    x = np.squeeze(X[k,:,:])
    M = np.dot(Y, np.transpose(Y))
    M = M-tempD
    M[np.where(M<0)]=0
    l = np.diag(np.sum(M,axis=0))-M
    L.append(l)
    CC.append(2* np.dot( np.dot(np.transpose(x),l), x ) )
  CC = np.array(CC)

  return CC

def f_lapLabelDistMatrix_2(X,G):
  K = X.shape[0]
  CC = []

  for k in range(0,K):
    l = nx.laplacian_matrix(G).toarray()
    CC.append(l)
  CC = np.array(CC)
  return CC

def multi_transpose(X):
  xt = []
  for i in range(0,X.shape[0]):
    x = np.squeeze(X[i,:,:])
    xt.append(np.transpose(x))
  Xt = np.array(xt)
  return Xt

def gradVal_eval(W,X,Y,rho_L3=None,C=None):
  y = np.squeeze(Y)
  grad_W = np.empty((X.shape[1],0),int)
  if rho_L3 == None:
    for i in range(0,X.shape[0]):
      x = np.squeeze(X[i,:,:])
      gloss = np.dot(x, np.dot(np.transpose(x),W[:,i])-y)
      gall = np.expand_dims(gloss, axis=1)
      grad_W = np.hstack((grad_W,gall))
  else:
    for i in range(0,X.shape[0]):
      x = np.squeeze(X[i,:,:])
      c = np.squeeze(C[i,:])
      gloss = np.dot(x, np.dot(np.transpose(x),W[:,i])-y)
      grho3 = rho_L3 * np.dot(c,W[:,i])
      gall = gloss+grho3
      gall = np.expand_dims(gall, axis=1)
      grad_W = np.hstack((grad_W,gall))
  return grad_W

def funVal_eval(W,X,Y,rho_L3=None,C=None):
  funcVal = 0
  y = np.squeeze(Y)
  for i in range (0,X.shape[0]):
    x = np.squeeze(X[i,:,:])
    funcVal = funcVal+0.5* (LA.norm(y-np.dot(np.transpose(x),W[:,i]))**2)
    #print(funcVal)
  if rho_L3 != None:
    for i in range (0,X.shape[0]):
      c = np.squeeze(C[i,:])
      funcVal = funcVal+0.5*rho_L3* np.dot(np.dot(np.transpose(W[:,i]),c),W[:,i])
    #print(funcVal)
  return funcVal

def nonsmooth_eval(W,rho1):
  non_smooth_value = 0
  for i in range(0,W.shape[0]):
    w = W[i,:]
    non_smooth_value = non_smooth_value + rho1* LA.norm(w)
    
  return non_smooth_value

def FGLasso_projection(W,lambda_):
  Wp = np.zeros(W.shape)

  for i in range(0,W.shape[0]):
    v = W[i,:]
    nm = LA.norm(v)
    if nm==0:
      w = np.zeros(v.shape)
    else:
      w = np.max(nm-lambda_, 0)/nm * v
    Wp[i,:] = np.transpose(w)
  
  return Wp

# Main function
def f_GMTM_APG(X, Y, rho1,rho_L3=None,C=None, max_iter = 1000, **kwargs):
  if 'verbose' not in kwargs:
        verbose = False
  else:
        verbose = kwargs['verbose']
  
  X = multi_transpose(X)
  #Y = multi_transpose(Y)
  task_num,dimension,nsubjects = X.shape  # Xt: modalities,ROIs,subjects
  
  W0_prep = np.empty((dimension,0),int)
  XY = []
  for t_idx in range(0,task_num):
    xy = np.dot(np.squeeze(X[t_idx,:,:]),Y).reshape(-1,1)
    XY.append(xy)
    W0_prep = np.hstack((W0_prep,xy))
  XY = np.array(XY)

  # initialize 0 as the starting point (search points)
  W0 = np.zeros((dimension,task_num))

  # False?
  bFlag = 0 #this flag tests whether the gradient step only changes a little

  Wz = W0   # solutions w
  Wz_old = W0

  t = 1
  t_old = 1

  #max_iter = 1000
  gamma = 1     # step size L
  gamma_inc = 2 # coeffcient 2^j*gamma
  value_gamma = np.zeros(int(max_iter))
  funcVal = np.zeros(int(max_iter)) #obj value

  for iter_step in range(int(max_iter)):

    # Compute serch point S based on Wz, Wz_old
    alpha = (t_old-1)/t
    Ws = (1+alpha)*Wz - alpha*Wz_old

    gWs = gradVal_eval(Ws,X,Y,rho_L3,C)
    Fs = funVal_eval(Ws,X,Y,rho_L3,C)

    while True:

      Wzp = FGLasso_projection(Ws - gWs/gamma, rho1/gamma)

      Fzp = funVal_eval(Wzp,X,Y,rho_L3,C) # F_k+1

      delta_Wzp = Wzp-Ws
      r_sum = LA.norm(delta_Wzp,'fro')**2
      
      # Q_k+1
      Fzp_gamma = Fs+np.trace( np.dot( np.transpose(delta_Wzp), gWs)) \
                   + gamma/2*LA.norm(delta_Wzp,'fro')**2
                  
      if (Fzp <= Fzp_gamma):
        break
      else:
        gamma = gamma*gamma_inc

    value_gamma[iter_step] = gamma
    Wz_old = Wz
    Wz = Wzp
    funcVal[iter_step] = Fzp+ nonsmooth_eval(Wz,rho1)
    
    t_old = t
    t = (1+math.sqrt(4*t*t+1))/2

    W = Wzp
    funcVal = np.array(funcVal)

    if bFlag is True:
      break

    # determin whether converge (change the tolerance?)
    if iter_step>2 and math.fabs(funcVal[iter_step]-funcVal[iter_step-1])< 1e-3:
      break

  return W

class L21Reg(BaseEstimator):
  def __init__(self, lambda1 = 0.1, lambda2 = None, max_iter = 1000):
    self.lambda1 = lambda1
    self.lambda2 = lambda2
    self.max_iter = max_iter
    self.coef_ = None
    self.idxPred = 0
  def fit(self, X, y):
    self.coef_ = f_GMTM_APG(X, y, rho1 = self.lambda1, max_iter = self.max_iter, verbose=False)
    return self
  def fitD(self, X, y, diag):
    c = f_lapLabelDistMatrix(X,diag)
    self.coef_ = f_GMTM_APG(X, y, rho1 = self.lambda1, rho_L3 = self.lambda2, C = c, max_iter = self.max_iter, verbose=False)
    return self
  def fitG(self, X, y, G):
    c = f_lapLabelDistMatrix_2(X,G)
    self.coef_ = f_GMTM_APG(X, y, rho1 = self.lambda1, rho_L3 = self.lambda2, C = c, max_iter = self.max_iter, verbose=False)
    return self
  
  def predict(self, X_test):
    if X_test.ndim == 3:
      predicted_y = [np.dot(np.squeeze(X_test[0,:,:]),self.coef_[:,0]), np.dot(np.squeeze(X_test[1,:,:]),self.coef_[:,1]), np.dot(np.squeeze(X_test[2,:,:]),self.coef_[:,2])]
    elif X_test.ndim == 2:
      predicted_y = np.dot(np.squeeze(X_test),self.coef_[:,self.idxPred])
    else:
      raise ValueError("Found array with dim %d. Expected = 2 or 3." % (X_test.ndim))
    return predicted_y

